{
  "trainingData": [
    {
      "id": 1,
      "inputCode": "import os\nfrom diagrams import Diagram, Cluster, Edge\nfrom diagrams.onprem.analytics import Spark\nfrom diagrams.onprem.compute import Server\nfrom diagrams.onprem.database import PostgreSQL\nfrom diagrams.onprem.inmemory import Redis\nfrom diagrams.onprem.aggregator import Fluentd\nfrom diagrams.onprem.monitoring import Grafana, Prometheus\nfrom diagrams.onprem.network import Nginx\nfrom diagrams.onprem.queue import Kafka\n\n# Create the Diagram instance with outformat=\"png\"\ndiagram = Diagram(\n    \"Advanced Web Services Open Source\",\n    outformat=\"png\",\n    show=not bool(os.environ.get('CI', 0))\n)\n\nwith diagram:\n    nginx = Nginx(\"Nginx Ingress\")\n\n    metrics = Prometheus(\"Promtheus metrics\")\n    metrics << Edge(color=\"firebrick\", style=\"dashed\") << Grafana(\"Grafana monitoring\")\n\n    with Cluster(\"Service Cluster\"):\n        grpcsvc = [\n            Server(\"grpc1\"),\n            Server(\"grpc2\"),\n            Server(\"grpc3\")\n        ]\n\n    with Cluster(\"Redis Sessions HA\"):\n        primary = Redis(\"session\")\n        primary \\\n            - Edge(color=\"brown\", style=\"dashed\") \\\n            - Redis(\"replica\") \\\n            << Edge(label=\"collect\") \\\n            << metrics\n        grpcsvc >> Edge(color=\"brown\") >> primary\n\n    with Cluster(\"PostgreSQL Database HA\"):\n        primary = PostgreSQL(\"users\")\n        primary \\\n            - Edge(color=\"brown\", style=\"dotted\") \\\n            - PostgreSQL(\"replica\") \\\n            << Edge(label=\"collect\") \\\n            << metrics\n        grpcsvc >> Edge(color=\"black\") >> primary\n\n    aggregator = Fluentd(\"Fluentd logging\")\n    aggregator \\\n        >> Edge(label=\"parse\") \\\n        >> Kafka(\"Kafka pub/sub stream\") \\\n        >> Edge(color=\"black\", style=\"bold\") \\\n        >> Spark(\"analytics\")\n\n    nginx \\\n        >> Edge(color=\"darkgreen\") \\\n        << grpcsvc \\\n        >> Edge(color=\"darkorange\") \\\n        >> aggregator\n",
      "outputText": "Consider switching to AWS ElastiCache or Azure Cache instead of Redis, if you want to reduce operational overhead or have outgrown a self-managed Redis cluster.\n Consider switching to Traefik or Envoy if you are already in a Kubernetes ecosystem, because you might need a solution with native auto-discovery, simpler TLS management, or advanced routing features.\n Consider switching from PostgreSQL Database HA to Amazon Aurora (PostgreSQL), Azure Database, or Google Cloud SQL, if you want to simplify high availability and scaling\n consider deploying gRPC services, Redis, PostgreSQL read replicas, Fluentd, Kafka, and Spark in Kubernetes or a managed container platform (AWS ECS/Fargate) for fine-grained resource management, horizontal autoscaling, rolling updates.\n consider using Web Application Firewall (WAF) if you face public traffic with potential malicious requests.\n For Kafka security, Use SSL/TLS encryption on all Kafka topics and Configure SASL (Simple Authentication and Security Layer) for client authentication and avoid storing sensitive data in plaintext messages. "
    },
    {
       "id": 2,
        "inputCode":"import os\nfrom diagrams import Diagram, Cluster\nfrom diagrams.aws.compute import ECS\nfrom diagrams.aws.database import RDS, ElastiCache\nfrom diagrams.aws.network import ELB, Route53\n\ngraph_attr = {\"splines\": \"spline\"}\n\nwith Diagram(\"AWS Clustered Web Services\", show=not bool(os.environ.get('CI', 0)), filename='images/aws_clustered_web_services'):\n    dns = Route53(\"Route53\")\n    lb = ELB(\"ELB\")\n\n    with Cluster(\"Web Services\"):\n        svc_group = [ECS(\"web2\"), ECS(\"web3\"), ECS(\"web1\")]\n\n    with Cluster(\"RDS Cluster\"):\n        db_primary = RDS(\"userdb\")\n        db_primary - [RDS(\"userdb ro\")]\n\n    memcached = ElastiCache(\"ElastiCache\\nmemcached\")\n\n    dns >> lb >> svc_group\n    svc_group >> db_primary\n    svc_group >> memcached",
        "outputText":"If you are using a classic ELB, consider upgrading to Application Load Balancer (ALB) for Layer 7 routing, path-based routing, and sticky sessions.\n If you need advanced routing or direct WebSocket support, ALB might offer more features than a classic ELB. Since you’re already on AWS, Amazon ElastiCache for Redis can reduce ops overhead compared to a self-managed Redis cluster.\n If “web1,” “web2,” and “web3” are on EC2, you might move them to ECS on Fargate or EKS (Kubernetes) for easier autoscaling and rolling updates.\nA serverless approach (e.g., AWS Lambda + API Gateway) is an option if the application is well-suited to event-driven workloads.\nConfigure scaling based on CPU, memory, or request counts so you can grow seamlessly to meet user demand.\n If you have global users, consider multi-region deployment or using Amazon CloudFront to reduce latency.\n Use CDN (CloudFront) for static content if you have large volumes of images, scripts, etc.\n Use AWS Trusted Advisor or third-party cost analysis tools to find inefficiencies.\n For non-critical workloads, consider EC2 Spot Instances to reduce compute costs.\n If your database workload is variable, Aurora Serverless can automatically scale to zero if there’s minimal traffic (e.g., off-hours).\n Deploy AWS WAF for layer 7 threat protection on the ELB or CloudFront distributions.\n Restrict database access to specific security groups; avoid public IPs for RDS unless absolutely necessary.\nStore database credentials and API keys in AWS Secrets Manager or Parameter Store."

    },
    {
        "id": 3,
        "inputCode":"import os\nfrom diagrams import Diagram, Cluster\nfrom diagrams.aws.compute import ECS, EKS, Lambda\nfrom diagrams.aws.database import Redshift\nfrom diagrams.aws.integration import SQS\nfrom diagrams.aws.storage import S3\n\ngraph_attr = {\"splines\": \"spline\"}\n\nwith Diagram(\"AWS Event Processing\", show=not bool(os.environ.get('CI', 0)), filename='images/aws_event_processing', graph_attr=graph_attr):\n    eks = EKS(\"EKS source\")\n\n    with Cluster(\"Event Flows\"):\n        with Cluster(\"Event Workers\"):\n workers = [ECS(\"ECS worker1\"), ECS(\"ECS worker2\"), ECS(\"ECS worker3\")]\n\n        queue = SQS(\"SQS event queue\")\n\n        with Cluster(\"Processing\"):\n            lambdas = [Lambda(\"Lambda proc2\"), Lambda(\"Lambda proc3\"), Lambda(\"Lambda proc1\")]\n\n    s3 = S3(\"S3 storage\")\n    redshift = Redshift(\"Redshift analytics\")\n\n    eks >> workers >> queue >> lambdas\n    lambdas >> s3\n    lambdas >> redshift",
        "outputText":"For ECS Workers, If these tasks are short-lived or scale rapidly, consider AWS Fargate (serverless containers) to eliminate the need for managing EC2 instances.\n To ensure your system scales with demand, configure autoscaling on ECS tasks and Lambda concurrency based on SQS queue depth, while employing dead-letter queues and visibility timeouts to manage back-pressure.\n Grouping resources into functional domains (production, queue, processing, and storage) clarifies how data flows from source to analytics, and including observability tools (e.g., CloudWatch logs and metrics) helps stakeholders quickly identify bottlenecks.\nTo reduce costs, right-size your workloads, leverage AWS Spot Instances for non-critical tasks, and explore cheaper storage tiers like S3 Infrequent Access or Glacier for rarely accessed data. \n  Security-wise, use strict IAM policies with roles for ECS/EKS and Lambda functions, enable encryption on SQS, S3, and Redshift with AWS KMS, and keep traffic private via VPC endpoints."
    },
    {
        "id":4,
        "inputCode":"import os\nfrom diagrams import Diagram\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.network import ELB\nfrom diagrams.onprem.client import Users\n\ngraph_attr = {\"splines\": \"spline\"}\n\nwith Diagram(\"AWS Load Balanced Web Farm\", show=not bool(os.environ.get('CI', 0)), direction=\"TB\", filename='images/aws_load_balanced_web_farm', graph_attr=graph_attr):\n    elb = ELB(\"ELB\nElastic Load Balancer\")\n\n    Users('Users') >> elb\n\n    elb >> [EC2(\"Web Server 1\"), EC2(\"Web Server 2\"), EC2(\"Web Server 3\"), EC2(\"Web Server 4\"), EC2(\"Web Server 5\")] >> RDS(\"RDS Database\")",
        "outputText":"replace any older “Classic” ELB with an Application Load Balancer for advanced routing rules built-in Layer 7 features.\nto migrate from legacy EC2 instances to Amazon ECS/Fargate or AWS Lambda if you want to reduce operational overhead and increase scalability.\nIf your current RDS database struggles with growing traffic, upgrading to Amazon Aurora or enabling read replicas can significantly improve performance under heavy loads. To make the architecture easier to understand, label each layer clearly (Load Balancer, Compute, Database) and possibly incorporate a caching tier (e.g., ElastiCache Redis) to reduce database load. Cost savings can be achieved by choosing Spot Instances for non-critical workloads or right-sizing each instance based on CPU/memory usage, while AWS WAF (Web Application Firewall) and appropriate IAM roles will help protect against common web threats and enforce least-privilege access."
    },
    {
        "id":5,
        "inputCode":"import os\nfrom diagrams import Diagram, Cluster\n\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.database import Dynamodb\nfrom diagrams.aws.integration import Appsync, StepFunctions\nfrom diagrams.aws.ml import Rekognition\nfrom diagrams.aws.security import Cognito\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.general import User\n\n# https://www.graphviz.org/doc/info/attrs.html\ngraph_attr = {\"splines\": \"spline\"}  # rounded arrows, much nicer\n\n# diagram name results in 'web_service.png' as the output name\n# pylint: disable=W0104,W0106\nwith Diagram('AWS Serverless Image Processing', show=not bool(os.environ.get('CI', 0)), filename='images/aws_serverless_image_processing', graph_attr=graph_attr):\n    appsync = Appsync(\"AWS AppSync\")\n\n    user = User(\"User\")\n\n    user >> Cognito(\"AWS Cognito\")\n\n    user >> S3(\"AWS S3 Bucket\") >> Lambda(\"Start workflow\") >> appsync\n\n    user >> appsync\n\n    resolver = Lambda(\"AWS Resolver\")\n\n    with Cluster(\"Backend\"):\n        dynamodb = Dynamodb(\"AWS DynamoDB\")\n        appsync >> resolver >> dynamodb\n\n        step_function = StepFunctions(\"AWS Step Function\")\n        resolver >> step_function\n\n        step_function >> Lambda(\"Create thumbnail\")\n\n        step_function >> Lambda(\"Persist Metadata\") >> dynamodb\n\n        step_function >> Lambda(\"Extract metadata\")\n\n        step_function >> Lambda(\"Object / Scene detection\") >> Rekognition(\"AWS Rekognition\")",
        "outputText":"Ensure you are using the latest AWS Lambda runtime (e.g., Python 3.10 or Node.js 18) and evaluating whether AppSync is still the best fit if you are only performing simple API calls (potentially shifting to API Gateway if GraphQL features are underused). If your user authentication needs are more advanced, you could consider updating your Cognito setup or switching to a provider like Auth0 for richer identity management. To improve performance and scaling, use AWS Step Functions for orchestrating multiple Lambdas in parallel, and set concurrency limits or provisioned concurrency where needed to handle surges in image upload requests. You can reduce costs further by monitoring S3 storage classes (e.g., transitioning infrequently accessed images to S3 Infrequent Access) and enabling auto-scaling for DynamoDB tables with on-demand capacity. From a security standpoint, enforce least-privilege IAM roles on each Lambda, enable server-side encryption for all S3 buckets, and review your network access patterns to ensure only authorized requests invoke the Step Functions or Rekognition APIs."
    },
    {
        "id":6,
        "inputCode": "import os\nfrom diagrams import Diagram, Cluster\nfrom diagrams.aws.compute import ECS\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.network import Route53\n\nwith Diagram(\"AWS Web Service DB Cluster\", show=not bool(os.environ.get('CI', 0)), filename='images/aws_web_service_db_cluster'):\n    Route53(\"Route53 DNS\") >> ECS(\"ECS web service\") >> RDS(\"primary\")\n\n    with Cluster(\"RDS DB Cluster\"):\n        RDS(\"primary\") - [RDS(\"replica2\"), RDS(\"replica1\")]",
        "outputText":"Replace any older container runtime with AWS Fargate or Amazon EKS for simpler autoscaling, reduced maintenance overhead, and improved container orchestration features. If the current RDS solution is older or experiencing performance bottlenecks, consider upgrading to Amazon Aurora (possibly Aurora Serverless) to handle higher traffic while scaling automatically and reducing operational costs. Labeling each layer clearly—DNS, Compute, and Database—will make the diagram more understandable to stakeholders, and implementing spot instances or right-sizing ECS tasks can further lower expenses. From a security perspective, ensure encryption at rest is enabled for all RDS instances, use TLS for connections, and enforce strict IAM roles for ECS tasks and DB access. Combining these optimizations "
    },
    {
        "id":7,
        "inputCode":"from diagrams import Diagram, Cluster\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.analytics import Glue, Athena\nimport diagrams.aws.ml as ml\nfrom diagrams.aws.integration import StepFunctions\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.network import APIGateway\nfrom IPython.display import Image\n\nwith Diagram(\"AWS Data Processing Pipeline\", show=False):\n\n    Lambda('Get Raw Data') >> S3(\"raw_data\")\n\n    with Cluster(\"Data Lake\"):\n        S3(\"raw_data\")\n        S3(\"staging_data\")\n        S3(\"data_capture\")\n\n    Athena(\"Athena\")\n    S3(\"raw_data\") >> Athena(\"Athena\")\n    S3(\"staging_data\") >> Athena(\"Athena\")\n    S3(\"data_capture\") >> Athena(\"Athena\")\n\n    with Cluster(\"Data Processing Pipeline\"):\n        StepFunctions(\"Pipeline\")\n        with Cluster(\"Glue Jobs\"):\n            Glue(\"job_data_quality\") >> Glue(\"job_data_transform\") >> Glue(\"job_dataset_model\")\n        S3(\"raw_data\") >> Glue(\"job_data_quality\")\n\n    with Cluster(\"SageMaker Model Deployment\"):\n        ml.SagemakerTrainingJob(\"job_train_model\") >> ml.SagemakerGroundTruth(\"job_evaluate_model\") >> ml.SagemakerModel(\"model_enpoint\")\n\n    APIGateway(\"API_gateway\")\n    Lambda(\"invoke_endpoint\")\n\n    S3(\"staging_data\") >> ml.SagemakerTrainingJob(\"job_train_model\") >> ml.SagemakerGroundTruth(\"job_evaluate_model\") >> ml.SagemakerModel(\"model_enpoint\")\n    ml.SagemakerModel(\"model_enpoint\") >> Lambda(\"invoke_endpoint\") >> APIGateway(\"API_gateway\")\n    ml.SagemakerModel(\"model_enpoint\") >> S3(\"data_capture\")\n    Glue(\"job_dataset_model\") >> ml.SagemakerTrainingJob(\"job_train_model\")",
        "outputText":"If any of your AWS Glue processes or Lambda functions are running on older runtimes or smaller instance types, consider upgrading to more recent AWS Glue versions (for Spark improvements) or higher Lambda memory/time configurations for faster data transformations. If Athena queries are slowing down, you could evaluate AWS EMR or Databricks for more robust analytics, especially if your data volumes are growing. Using AWS Lake Formation on top of S3 can streamline access control and governance to make the environment more understandable for data consumers. To handle higher traffic for your SageMaker endpoint, enable autoscaling on the model endpoint or switch to SageMaker Serverless Inference to dynamically scale concurrency. Keep costs in check by right-sizing Glue jobs, using on-demand scheduling, and storing infrequently accessed data in S3 Infrequent Access or Glacier. Finally, for cybersecurity, enforce encryption at rest (KMS for S3 buckets, data in transit for SageMaker endpoints, and mutual TLS where possible), apply IAM roles with least privilege for jobs, and regularly rotate credentials to prevent unauthorized data access."
    },
    {
        "id":8,
        "inputCode":"from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.database import Aurora, DDB\nfrom diagrams.aws.network import VPC, CF, ELB, InternetGateway, NATGateway\nfrom diagrams.aws.storage import S3\n\nwith Diagram(\"AWS Simple Architecture\", show=False, outformat=\"png\"):\n    CF(\"CloudFront\") >> InternetGateway(\"IGW\") >> ELB(\"ALB\") >> [EC2(\"app1\"), EC2(\"app2\"), EC2(\"app3\")]\n\n    with Cluster(\"VPC\"):\n        with Cluster(\"Private Subnet\"):\n            with Cluster(\"App\"):\n                [EC2(\"app1\"), EC2(\"app2\"), EC2(\"app3\")]\n            with Cluster(\"Aurora Cluster\"):\n                Aurora(\"Writer\") - Aurora(\"Reader\")\n        with Cluster(\"Public Subnet\"):\n            EC2(\"Bastion\") >> Edge(label=\"login\") >> EC2(\"app1\")\n\n    EC2(\"app1\") >> Aurora(\"Writer\")",
        "outputText":"Consider replacing your traditional bastion host with AWS Systems Manager Session Manager for remote access, which removes the need for publicly exposed SSH and simplifies security. If your application servers are currently on older EC2 instances, migrating them to AWS ECS/EKS (containers) or AWS Lambda (serverless) could reduce operational overhead and scale more seamlessly during traffic spikes. Introducing AWS WAF on top of CloudFront can help protect against common web exploits, and Amazon Aurora Serverless can automatically adjust capacity for your database to handle unpredictable workloads while keeping costs in check. Labeling each tier (edge, public subnet, private subnet, DB cluster) more explicitly in the diagram can make it easier for stakeholders to understand data flows and trust boundaries. Ensure end-to-end TLS (HTTPS) is enforced from CloudFront through the ALB to your apps, enable encryption at rest for Aurora, and apply least-privilege IAM roles to minimize cybersecurity risks."
    },
    {
        "id": 9,
        "inputCode":"from diagrams import Cluster, Diagram\nfrom diagrams.aws.compute import ECS, EKS, Lambda\nfrom diagrams.aws.database import Redshift\nfrom diagrams.aws.integration import SQS\nfrom diagrams.aws.storage import S3\n\nwith Diagram(\"Event Processing\", show=False):\n    EKS(\"k8s source\") >> [ECS(\"worker1\"), ECS(\"worker2\"), ECS(\"worker3\")] >> SQS(\"event queue\") >> [Lambda(\"proc1\"), Lambda(\"proc2\"), Lambda(\"proc3\")]\n\n    with Cluster(\"Event Flows\"):\n        with Cluster(\"Event Workers\"):\n            [ECS(\"worker1\"), ECS(\"worker2\"), ECS(\"worker3\")]\n        with Cluster(\"Processing\"):\n            [Lambda(\"proc1\"), Lambda(\"proc2\"), Lambda(\"proc3\")]\n\n    [Lambda(\"proc1\"), Lambda(\"proc2\"), Lambda(\"proc3\")] >> S3(\"events store\")\n    [Lambda(\"proc1\"), Lambda(\"proc2\"), Lambda(\"proc3\")] >> Redshift(\"analytics\")",
        "outputText":"Consider upgrading any legacy container tasks to fully managed solutions such as AWS Fargate if you want to reduce the operational overhead of managing worker nodes in a Kubernetes environment. If your event queue is self-managed or an older queue system, Amazon SQS or Amazon MSK (Kafka) may provide more reliability and easier scaling for high-throughput scenarios. To further optimize performance, you can enable Lambda concurrency controls so that proc1, proc2, and proc3 each scale automatically based on message volume, while setting appropriate batch sizes to reduce overhead. If cost is a concern, explore spot instances for your K8s workers (or Fargate Spot), and choose lower-cost storage tiers such as S3 Infrequent Access for historical data. For clarity, label each stage—source, workers, queue, processors, storage, analytics—in the diagram and use consistent color coding or icons. Finally, to address cybersecurity, apply least-privilege IAM roles so workers and Lambda functions only access necessary resources, enable encryption at rest for event storage and analytics data, and consider AWS WAF or similar protective measures if external inputs might introduce malicious payloads."
    },
    {
        "id": 10,
        "inputCode":"from diagrams import Cluster, Diagram\nfrom diagrams.gcp.analytics import BigQuery, Dataflow, PubSub\nfrom diagrams.gcp.compute import AppEngine, Functions\nfrom diagrams.gcp.database import BigTable\nfrom diagrams.gcp.iot import IotCore\nfrom diagrams.gcp.storage import GCS\n\nwith Diagram(\"Message Collecting\", show=False):\n    PubSub(\"pubsub\")\n\n    with Cluster(\"Source of Data\"):\n        [IotCore(\"core1\"), IotCore(\"core2\"), IotCore(\"core3\")] >> PubSub(\"pubsub\")\n\n    with Cluster(\"Targets\"):\n        with Cluster(\"Data Flow\"):\n            Dataflow(\"data flow\")\n        with Cluster(\"Data Lake\"):\n            Dataflow(\"data flow\") >> [BigQuery(\"bq\"), GCS(\"storage\")]\n        with Cluster(\"Event Driven\"):\n            with Cluster(\"Processing\"):\n                Dataflow(\"data flow\") >> AppEngine(\"engine\") >> BigTable(\"bigtable\")\n            with Cluster(\"Serverless\"):\n                Dataflow(\"data flow\") >> Functions(\"func\") >> AppEngine(\"appengine\")\n\n    PubSub(\"pubsub\") >> Dataflow(\"data flow\")",
        "outputText":"If you are running older versions of Dataflow or using outdated App Engine runtimes, consider migrating to the latest Apache Beam SDK versions in Dataflow or moving parts of the application to Cloud Run for simpler container-based deployments. To handle scaling more seamlessly, ensure Pub/Sub subscriptions have sufficient throughput and use horizontal autoscaling for your processing functions or App Engine services. If you see higher than expected latency or cost in Bigtable, review schema design or evaluate Cloud Spanner for multi-region consistency. For better observability, label each stage in the diagram more clearly, including how data moves from Pub/Sub to Dataflow, then on to BigQuery for long-term analytics or storage for archival. Cost savings can come from shifting infrequently accessed data to lower-cost storage tiers, and using Cloud Functions with pay-per-invocation pricing rather than always-on instances. In terms of cybersecurity, apply strict IAM permissions for each service, encrypt data at rest in Bigtable and BigQuery, and ensure TLS is enforced when data is transferred between services."
    },
    {
        
        "id": 11,
        "inputCode":"from diagrams import Diagram\nfrom diagrams.k8s.clusterconfig import HPA\nfrom diagrams.k8s.compute import Deployment, Pod, ReplicaSet\nfrom diagrams.k8s.network import Ingress, Service\n\nwith Diagram(\"Exposed Pod with 3 Replicas\", show=False):\n    Ingress(\"domain.com\") >> Service(\"svc\") >> [Pod(\"pod1\"), Pod(\"pod2\"), Pod(\"pod3\")] << ReplicaSet(\"rs\") << Deployment(\"dp\") << HPA(\"hpa\")",
        "outputText":"If you are using an older ingress controller or a traditional load balancer, consider switching to a more modern solution such as Traefik or Envoy for improved routing and streamlined TLS configuration. If your pods are running on an older container runtime, upgrading to containerd or CRI-O can boost performance and reduce overhead. To handle more user traffic, configure the horizontal pod autoscaler with resource-based metrics and possibly adopt cluster autoscaling for on-demand node provisioning. Labeling each Kubernetes object clearly (ingress, service, replica set, deployment, and pods) will make the architecture easier for teams to understand and troubleshoot. You can also lower costs by using a managed Kubernetes service with spot instances or by moving stateless workloads to serverless options if suitable. From a security perspective, enforce strict role-based access control for cluster operations, consider applying network policies to restrict pod-to-pod communication, and make sure all traffic is secured via TLS."
    },
    {
        
        "id": 12,
        "inputCode":"from diagrams import Cluster, Diagram\nfrom diagrams.k8s.compute import Pod, StatefulSet\nfrom diagrams.k8s.network import Service\nfrom diagrams.k8s.storage import PV, PVC, StorageClass\n\nwith Diagram(\"Stateful Architecture\", show=False):\n    with Cluster(\"Apps\"):\n        Service(\"svc\") >> [Pod(\"pod\") - StatefulSet(\"sts\") - PVC(\"pvc\") for _ in range(3)]\n    [Pod(\"pod\") >> PVC(\"pvc\") for _ in range(3)] << PV(\"pv\") << StorageClass(\"sc\")",
        "outputText":"If your storage mechanism relies on older volume plugins or manual provisioning, you may consider switching to a newer CSI driver with dynamic provisioning to simplify scaling and improve performance. Using a StatefulSet is effective for workloads needing stable identifiers and persistent data, but you could optimize further with pod anti-affinity rules and increased replica counts if the application supports horizontal scaling. Clarifying which pods map to which persistent volume claims and specifying the relationship between the storage class and persistent volumes can make the architecture more approachable for new contributors. Costs can be reduced by assessing how much storage is truly needed, right-sizing volumes accordingly, and adopting a managed service for persistent storage if your cloud provider offers one. From a security perspective, enforce least-privilege access to the underlying storage, enable encryption at rest wherever possible, and consider network policies or a zero-trust approach to ensure only approved pods can access sensitive data."
    },
    {
    "id": 13,
    "inputCode":"from urllib.request import urlretrieve\n\nfrom diagrams import Cluster, Diagram\nfrom diagrams.aws.database import Aurora\nfrom diagrams.custom import Custom\nfrom diagrams.k8s.compute import Pod\n\nrabbitmq_url = \"https://jpadilla.github.io/rabbitmqapp/assets/img/icon.png\"\nrabbitmq_icon = \"rabbitmq.png\"\nurlretrieve(rabbitmq_url, rabbitmq_icon)\n\nwith Diagram(\"Broker Consumers\", show=False):\n    with Cluster(\"Consumers\"):\n        [Pod(\"worker\"), Pod(\"worker\"), Pod(\"worker\")]\n    Custom(\"Message queue\", rabbitmq_icon) >> [Pod(\"worker\"), Pod(\"worker\"), Pod(\"worker\")] >> Aurora(\"Database\")",
    "outputText":"If you are running an older RabbitMQ version or managing it manually, consider using a newer managed service such as CloudAMQP or another hosted queue solution to reduce operational overhead. You could also migrate your consumers to a container platform like Kubernetes or a serverless environment if the current system is difficult to scale, allowing you to add more worker pods automatically during peak loads. Labeling each step clearly (producer, queue, consumer pods, database) could help other stakeholders quickly understand the data flow. To optimize costs, right-size compute resources for the consumer pods, monitor message throughput, and scale down during quieter periods. Finally, ensure you are using encrypted connections to RabbitMQ and the database, enforce IAM or role-based access control for user credentials, and regularly patch or update RabbitMQ to address any known vulnerabilities."
    },
    {
        
        "id": 14,
        "inputCode":"from diagrams import Diagram, Cluster\nfrom diagrams.aws.network import Route53, ELB, APIGateway\nfrom diagrams.aws.compute import EC2AutoScaling\nfrom diagrams.aws.integration import SQS\nfrom diagrams.aws.analytics import KinesisDataStreams\nfrom diagrams.aws.database import Dynamodb, Redshift\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.ml import ElasticInference\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.aws.security import WAF\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.devtools import Codebuild\nfrom diagrams.aws.general import User\nfrom diagrams.aws.database import ElastiCache\nfrom diagrams.onprem.compute import Server\n\nwith Diagram(\"AWS Scalable Architecture\", show=True, direction=\"LR\"):\n    User(\"User\") >> Route53(\"Route 53\") >> ELB(\"ELB\") >> [EC2AutoScaling(\"Front (Auto-Scaling Multi-AZ)\"), EC2AutoScaling(\"Logger (Auto-Scaling Multi-AZ)\")]\n\n    with Cluster(\"Across all AWS Regions\"):\n        EC2AutoScaling(\"Front (Auto-Scaling Multi-AZ)\")\n        EC2AutoScaling(\"Logger (Auto-Scaling Multi-AZ)\")\n    \n    KinesisDataStreams(\"Kinesis\") >> EC2AutoScaling(\"Worker (Auto-Scaling Multi-AZ)\") >> ElastiCache(\"Redis\")\n    \n    EC2AutoScaling(\"Worker (Auto-Scaling Multi-AZ)\") >> SQS(\"SQS\") >> ElasticInference(\"Inserter\") >> [S3(\"S3\"), Redshift(\"Redshift\")]\n\n    with Cluster(\"State Sharing Configuration\"):\n        Dynamodb(\"DynamoDB\") >> Lambda(\"Lambda\") >> APIGateway(\"API Gateway\") >> ELB(\"ELB\")",
        "outputText":"If your existing configuration uses older EC2 instances for the front or logger components, consider migrating them to AWS Fargate or Lambda functions for reduced operational overhead and simpler scaling. You could also upgrade any legacy Kinesis streams to newer, enhanced fan-out models or evaluate Amazon MSK (Kafka) if you need stronger guarantees or larger throughput. Labeling each layer more distinctly, such as “inbound DNS routing,” “API Gateway and Lambda for state sharing,” “front-end auto-scaling tier,” “stream processing,” “caching,” and “data ingestion” will make the system easier to understand for new contributors. For cost optimization, right-size your auto-scaling groups, enable on-demand or spot instances for stateless workloads, and store infrequently accessed data in lower-cost S3 tiers. From a security standpoint, enforce encryption in transit (TLS) on all traffic between components, apply IAM policies with least-privilege access, and keep an eye on potential injection vulnerabilities if the logger or workers handle untrusted data."
    },
    {
        
        "id": 15,
        "inputCode":"from diagrams import Diagram, Cluster\nfrom diagrams.aws.network import ELB\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.general import User\n\nwith Diagram(\"AWS High-Availability Architecture\", show=True, direction=\"LR\"):\n    User(\"User\") >> ELB(\"Load Balancer\") >> [EC2(\"Web Server\"), EC2(\"Web Server\")]\n    ELB(\"Load Balancer\") >> ELB(\"Load Balancer\") >> [EC2(\"App Server\"), EC2(\"App Server\")]\n\n    with Cluster(\"AZ1\"):\n        EC2(\"Web Server\")\n        EC2(\"App Server\")\n        RDS(\"Primary Database\")\n    \n    with Cluster(\"AZ2\"):\n        EC2(\"Web Server\")\n        EC2(\"App Server\")\n        RDS(\"Secondary Database\")\n    \n    [EC2(\"App Server\"), EC2(\"App Server\")] >> RDS(\"Primary Database\")\n    RDS(\"Primary Database\") >> RDS(\"Secondary Database\")",
        "outputText":"If you are using older classic load balancers, consider replacing them with Application Load Balancers for more advanced routing and potential cost savings. You could also migrate the app servers to a containerized environment such as ECS or EKS for simplified scaling and management. Labeling each tier distinctly, such as “Web Tier,” “App Tier,” and “Database Tier,” helps new teams quickly understand how traffic flows and where data is stored. To handle more users without downtime, ensure auto-scaling groups are in place across multiple Availability Zones, and possibly switch to Amazon Aurora for the database if your existing solution struggles with performance or high availability. You can reduce operational costs by right-sizing instances, using reserved instances or savings plans, and shutting down underutilized development environments. Finally, keep all traffic encrypted with TLS from the load balancers to the servers, enforce strict IAM roles and security group rules, and patch both the operating system and database engines regularly to address potential vulnerabilities."
    },
    {
        "id":16,
        "inputCode":"from diagrams import Diagram, Cluster\nfrom diagrams.aws.network import VPC, InternetGateway, CloudFront, Route53, VPCRouter\nfrom diagrams.aws.security import IAM\nfrom diagrams.aws.compute import EC2AutoScaling, EC2\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.engagement import SES\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.aws.general import User\n\nwith Diagram(\"AWS Cloud Architecture\", show=True, direction=\"LR\"):\n    User(\"Users\") >> CloudFront(\"CloudFront CDN\") >> Route53(\"Route 53\")\n    SES(\"Amazon SES\") >> User(\"Users\")\n\n    with Cluster(\"Virtual Private Cloud (VPC)\"):\n        CloudFront(\"CloudFront CDN\") >> InternetGateway(\"VPC NAT Gateway\") >> VPCRouter(\"VPC Router\")\n        with Cluster(\"DMZ (Public Subnet)\"):\n            EC2(\"Bastion Host\")\n        with Cluster(\"Availability Zone 1\"):\n            EC2AutoScaling(\"Auto Scaling Group\") >> EC2(\"Amazon EC2\") >> RDS(\"Primary RDS DB\")\n            EC2(\"Amazon EC2\") >> S3(\"S3 Storage\")\n        with Cluster(\"Availability Zone 2\"):\n            EC2AutoScaling(\"Auto Scaling Group\") >> EC2(\"Amazon EC2\") >> RDS(\"Replica RDS DB\")\n            EC2(\"Amazon EC2\") >> S3(\"S3 Storage\")\n        Cloudwatch(\"CloudWatch Logs\")\n        IAM(\"IAM Role\") >> [EC2(\"Amazon EC2\"), EC2(\"Amazon EC2\")]\n        RDS(\"Primary RDS DB\") >> RDS(\"Replica RDS DB\")",
        "outputText":"If your auto scaling groups still rely on manual instance management, you could consider moving parts of your workload to AWS Fargate or AWS Lambda for reduced operational overhead and simpler scaling. You might also switch any older RDS instances to Amazon Aurora for faster failover and improved performance in multi-AZ deployments. Labeling each tier clearly (web, app, database, CDN) can make it easier for teammates to understand how traffic flows and where data persists. To optimize costs, right-size your EC2 instances, utilize savings plans, and monitor storage usage in S3 to offload infrequently accessed data to cheaper tiers. Finally, maintain security by enforcing IAM role best practices, encrypting all data at rest and in transit, and limiting inbound traffic to the bastion host or NAT gateway for secure management."
        
    },
    {
        "id":17,
        "inputCode":"from diagrams import Diagram, Cluster\nfrom diagrams.aws.network import APIGateway\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.database import Dynamodb, RDS\nfrom diagrams.aws.security import IAM\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.database import ElastiCache\nfrom diagrams.generic.device import Mobile\n\nwith Diagram(\"AWS Mobile Backend\", show=True, direction=\"LR\"):\n    Mobile(\"Mobile App\") >> APIGateway(\"Amazon API Gateway\")\n\n    with Cluster(\"VPC\"):\n        APIGateway(\"Amazon API Gateway\") >> Lambda(\"AWS Lambda Functions\\n(User Identity Management)\")\n        APIGateway(\"Amazon API Gateway\") >> Lambda(\"AWS Lambda Functions\\n(Core Business Logic)\")\n        Lambda(\"AWS Lambda Functions\\n(Core Business Logic)\") >> RDS(\"Amazon RDS\")\n        Lambda(\"AWS Lambda Functions\\n(Core Business Logic)\") >> ElastiCache(\"Amazon ElastiCache\")\n        Lambda(\"AWS Lambda Functions\\n(Core Business Logic)\") >> S3(\"Amazon S3\")\n    IAM(\"Amazon Cognito\")\n    Lambda(\"AWS Lambda Functions\\n(User Identity Management)\") >> IAM(\"Amazon Cognito\")\n    Lambda(\"AWS Lambda Functions\\n(User Identity Management)\") >> Dynamodb(\"Amazon DynamoDB\")",
        "outputText":"If you are using older Lambda runtimes or find your relational database struggling with increasing traffic, consider upgrading to newer runtimes (for example, Node.js 18 or Python 3.10) and migrating to Amazon Aurora Serverless for automated scaling. You could also evaluate DynamoDB for more of your data if you need serverless, high-throughput reads and writes. Labeling each function more explicitly (for example, indicating which ones handle authentication versus which handle business logic) will make the architecture clearer to your team. To reduce costs, monitor Lambda concurrency and configure provisioned concurrency only for critical paths, while transitioning infrequently accessed data to lower-cost S3 storage tiers. Finally, ensure all traffic from API Gateway to Lambda is using IAM roles with least-privilege access, encrypt ElastiCache connections if sensitive data is cached, and enable AWS WAF or Cognito authentication to help protect against unauthorized requests."  
    },
    {
        "id":18,
        "outputText": "To optimize this architecture, consider using AWS Fargate for containerized applications instead of EC2 for better scalability.\nReplace Amazon RDS with Aurora for auto-scaling capabilities.\nUse a CDN like CloudFront to serve media files efficiently.\nConsider using Redis over Memcached in ElastiCache for better persistence and clustering.\nEnsure proper IAM policies to prevent unauthorized access to AWS resources.",
    "inputCode":"from diagrams import Diagram, Cluster\\nfrom diagrams.aws.compute import EC2\\nfrom diagrams.aws.database import RDS\\nfrom diagrams.aws.network import ELB\\nfrom diagrams.aws.integration import SQS\\nfrom diagrams.onprem.client import User\\nfrom diagrams.onprem.compute import Server\\nfrom diagrams.onprem.database import PostgreSQL\\nwith Diagram(\"System Architecture\", show=False):\\n    user = User(\"User\")\\n    with Cluster(\"Load Balancing Layer\"):\\n        lb = ELB(\"Load Balancer\")\\n    with Cluster(\"Application Layer\"):\\n        app1 = EC2(\"App Server 1\")\\n        app2 = EC2(\"App Server 2\")\\n    with Cluster(\"Database Layer\"):\\n        db = RDS(\"Database\")\\n    user >> lb >> [app1, app2]\\n    app1 >> db\\n    app2 >> db"
     
    },
    { "id":19,
    "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.network import VPC, InternetGateway, NATGateway, RouteTable\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.database import Dynamodb\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.security import Shield\nfrom diagrams.aws.general import User\nfrom diagrams.onprem.network import VPN\n\nwith Diagram(\"AWS VPC Architecture\", show=False, direction=\"TB\"):\n    internet = InternetGateway(\"Internet Gateway\")\n    vpc = VPC(\"VPC 10.0.0.0/16\")\n\n    with Cluster(\"Public VPC Subnet 10.0.0.0/24\"):\n        nat_gateway = NATGateway(\"NAT Gateway\\n10.0.1.31\")\n        instance1 = EC2(\"Instance 10.0.1.10\")\n        route_table_pub = RouteTable(\"Route Table\")\n\n    with Cluster(\"Private VPC Subnet 10.0.1.0/24\"):\n        instance2 = EC2(\"Instance 10.0.1.11\")\n        instance3 = EC2(\"Instance 10.0.1.12\")\n        route_table_priv = RouteTable(\"Route Table\")\n\n    dynamo = Dynamodb(\"Amazon DynamoDB\")\n    s3 = S3(\"S3 Bucket\")\n\n    vpn_gateway = VPN(\"VPN Gateway\")\n    corp_data_center = User(\"Corporate Data Center\")\n\n    internet >> nat_gateway >> instance1\n    instance1 >> route_table_pub\n    nat_gateway >> instance2\n    nat_gateway >> instance3\n    instance2 >> route_table_priv\n    instance3 >> route_table_priv\n\n    instance1 >> Edge(label=\"Data Flow\") >> dynamo\n    instance2 >> Edge(label=\"Data Storage\") >> s3\n\n    vpn_gateway >> corp_data_center\n\n    route_table_priv >> vpn_gateway\n    route_table_pub >> internet",
    "outputText": "Consider replacing NAT Gateway with AWS Transit Gateway for better scalability and cost-effectiveness. \n    If security and performance are critical, consider AWS Direct Connect for a dedicated private connection. \n    Use Auto Scaling Groups for EC2 instances to dynamically adjust resources based on traffic demands. \n    Consider using AWS Global Accelerator to reduce latency and improve application performance. \n    Switch to AWS Savings Plans or Reserved Instances to reduce EC2 instance costs over time. \n    Ensure all data is encrypted in transit and at rest. Use AWS WAF and Shield for added security protection. \n    Use AWS CloudFormation templates or Terraform for better infrastructure as code (IaC) management."
    },
    {
        "id":20,
        "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.network import VPC, InternetGateway, RouteTable\nfrom diagrams.aws.compute import EC2, AutoScaling\nfrom diagrams.aws.security import Shield\nfrom diagrams.aws.database import RDS, Dynamodb\nfrom diagrams.aws.analytics import DataPipeline\nfrom diagrams.aws.general import User\nfrom diagrams.aws.storage import S3\nfrom diagrams.onprem.network import Internet\n\nwith Diagram(\"AWS Architecture Diagram\", show=False, direction=\"TB\"):\n    internet = Internet(\"Internet Connection\")\n    vpc = VPC(\"AWS VPC\")\n    igw = InternetGateway(\"Internet Gateway\")\n\n    with Cluster(\"VPC Public Subnet\"):\n        web_firewall_1 = Shield(\"Web App Firewall\")\n        web_firewall_2 = Shield(\"Web App Firewall\")\n        web_server_1 = EC2(\"Elastic Load Balancer\")\n        web_server_2 = EC2(\"Elastic Load Balancer\")\n    \n    with Cluster(\"VPC Private Subnet\"):\n        autoscaling = AutoScaling(\"Auto Scaling\")\n        compute_instances = EC2(\"Compute Instances\")\n\n    with Cluster(\"VPC Private Subnet - Databases\"):\n        database_mysql = RDS(\"MySQL\")\n        database_dynamo = Dynamodb(\"Amazon DynamoDB\")\n        storage = S3(\"Data Storage\")\n\n    internet >> igw >> web_firewall_1 >> web_server_1\n    internet >> igw >> web_firewall_2 >> web_server_2\n    web_server_1 >> autoscaling >> compute_instances\n    web_server_2 >> autoscaling >> compute_instances\n    compute_instances >> database_mysql\n    compute_instances >> database_dynamo\n    database_mysql >> storage\n    database_dynamo >> storage",
        "outputText": "Consider replacing traditional EC2 instances with AWS Fargate for serverless containerized workloads to improve scalability and reduce management overhead. \nUse AWS Shield Advanced for enhanced protection against DDoS attacks. \nUtilize AWS Global Accelerator to optimize traffic routing and reduce latency. \nImplement AWS Secrets Manager to securely manage database credentials. \nSwitch from RDS MySQL to Amazon Aurora for better performance and cost efficiency. \nUse S3 Intelligent-Tiering for storage cost savings. \nAdopt Infrastructure as Code (IaC) using AWS CloudFormation or Terraform for better automation and resource management."
    },
    {
        "id":21,
         "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import Fargate, Lambda\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.integration import SQS\nfrom diagrams.aws.database import Dynamodb\n\nwith Diagram(\"AWS Cloud Architecture\", show=False, direction=\"LR\"):\n    fargate = Fargate(\"AWS Fargate\")\n    bucket = S3(\"Bucket\")\n    glacier = S3(\"S3 Glacier Deep Archive\")\n    lambda_query = Lambda(\"AWS Lambda\")\n    lambda_restore = Lambda(\"AWS Lambda\")\n    sqs = SQS(\"Amazon Simple Queue Service (Amazon SQS)\")\n    dynamodb = Dynamodb(\"Amazon DynamoDB\")\n\n    fargate >> bucket >> lambda_query >> glacier\n    lambda_query >> lambda_restore >> sqs\n    sqs >> dynamodb\n",
            
        "outputText": "Consider replacing AWS Lambda with AWS Step Functions to better orchestrate workflows and reduce execution time. Use AWS EventBridge instead of SQS if event-driven architecture is required for better scalability. Implement AWS IAM policies to ensure secure access control to AWS Fargate and Lambda functions. Use Amazon RDS instead of DynamoDB if relational data is required for complex querying needs. Enable S3 Lifecycle Policies to automate Glacier Deep Archive transitions for cost efficiency. Utilize AWS X-Ray for end-to-end monitoring and debugging of the system."
          
          
    },
    {
        "id":22,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.network import VPC, InternetGateway, Route53, CloudFront\nfrom diagrams.aws.compute import EC2, AutoScaling, Lambda\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.security import Shield\nfrom diagrams.aws.general import User\nfrom diagrams.aws.storage import S3\n\nwith Diagram(\"AWS Large Scale Architecture\", show=False, direction=\"TB\"):\n    vpc = VPC(\"VPC\")\n    route53 = Route53(\"Route 53\")\n    cloudfront = CloudFront(\"CloudFront\")\n    internet_gateway = InternetGateway(\"Internet Gateway\")\n\n    with Cluster(\"Web Tier\"):\n        web_servers = [EC2(\"Website-1\"), EC2(\"Website-2\"), EC2(\"Website-3\")]\n        load_balancer = Lambda(\"Load Balancer\")\n        auto_scaling = AutoScaling(\"AutoScaling Group\")\n\n    with Cluster(\"Application Tier\"):\n        app_servers = [EC2(\"App Server 1\"), EC2(\"App Server 2\"), EC2(\"App Server 3\")]\n        app_auto_scaling = AutoScaling(\"AutoScaling Group\")\n\n    with Cluster(\"Database Tier\"):\n        primary_db = RDS(\"Primary SQL DB\")\n        replica_db = RDS(\"Replica SQL DB\")\n\n    storage = S3(\"Data Storage\")\n\n    route53 >> cloudfront >> internet_gateway >> load_balancer >> web_servers\n    web_servers >> auto_scaling >> app_servers\n    app_servers >> app_auto_scaling >> primary_db\n    primary_db >> replica_db\n    primary_db >> storage\n",
            
            "outputText": "Consider using AWS Elastic Load Balancer (ALB) instead of Lambda for load balancing to optimize performance. \nUtilize AWS WAF for enhanced security protection at the web tier. \nImplement AWS RDS Multi-AZ deployment for improved database reliability and failover support. \nUse Amazon ElastiCache to optimize application-tier performance and reduce database load. \nEnable CloudFront caching to reduce latency and offload requests from web servers. \nAdopt AWS Systems Manager for automated application management and monitoring."
          
          
    },
    {
        "id":23,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.analytics import Kinesis\nfrom diagrams.aws.database import Dynamodb\nfrom diagrams.aws.ml import Sagemaker\nfrom diagrams.aws.iot import IotCore\nfrom diagrams.aws.integration import SQS\nfrom diagrams.aws.storage import S3\n\nwith Diagram(\"AWS Connected Vehicle Architecture\", show=False, direction=\"LR\"):\n    iot_core = IotCore(\"AWS IoT Core\")\n    raw_data = S3(\"Raw Data Storage\")\n\n    with Cluster(\"Data Processing\"):\n        kinesis_trip = Kinesis(\"Trip Data Stream\")\n        kinesis_anomaly = Kinesis(\"Anomaly Detection Stream\")\n        kinesis_safety = Kinesis(\"Driver Safety Score Stream\")\n        lambda_trip = Lambda(\"Trip Data Processing\")\n        lambda_anomaly = Lambda(\"Anomaly Detection\")\n        lambda_safety = Lambda(\"Safety Score Calculation\")\n\n    with Cluster(\"Storage & Notification\"):\n        dynamodb_anomaly = Dynamodb(\"Anomaly Data Store\")\n        dynamodb_safety = Dynamodb(\"Safety Scores Store\")\n        dynamodb_marketing = Dynamodb(\"Location Marketing Data\")\n        sns_notifications = SQS(\"Notification Service\")\n\n    iot_core >> raw_data >> [kinesis_trip, kinesis_anomaly, kinesis_safety]\n    kinesis_trip >> lambda_trip >> dynamodb_safety\n    kinesis_anomaly >> lambda_anomaly >> dynamodb_anomaly\n    kinesis_safety >> lambda_safety >> dynamodb_safety\n    lambda_safety >> sns_notifications\n    lambda_anomaly >> sns_notifications\n",
            
            "outputText": "Consider using AWS IoT Analytics for more scalable and managed IoT data processing. \nImplement AWS Lambda function optimization by reducing execution time and cold starts. \nUse AWS EventBridge instead of SQS for better event-driven integration. \nEnable AWS WAF and Shield to secure the IoT data pipeline from malicious traffic. \nOptimize DynamoDB costs by implementing on-demand capacity mode for unpredictable workloads. \nUtilize AWS Step Functions to orchestrate Lambda workflows for better maintainability."
          
          
    },
    {
        "id":24,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.network import VPC, InternetGateway, NATGateway, Router\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.general import Client\n\nwith Diagram(\"AWS SAP HANA Architecture\", show=False, direction=\"TB\"):\n    vpc = VPC(\"VPC\")\n    internet_gateway = InternetGateway(\"Internet Gateway\")\n    nat_gateway = NATGateway(\"NAT Gateway\")\n    router = Router(\"Router\")\n\n    with Cluster(\"Public Subnet\"):\n        hana_studio = EC2(\"HANA Studio\")\n        bastion = EC2(\"Bastion Host\")\n\n    with Cluster(\"Private Subnet - Virtual Private Cloud\"):\n        sap_hana = RDS(\"SAP HANA Database\")\n        backup_snapshots = S3(\"Backup Snapshots\")\n        hana_logs = S3(\"HANA Log Storage\")\n        hana_media = S3(\"HANA Media Storage\")\n        hana_data = S3(\"HANA Data Storage\")\n\n    internet_gateway >> router >> [hana_studio, bastion]\n    nat_gateway >> sap_hana\n    sap_hana >> [backup_snapshots, hana_logs, hana_media, hana_data]\n",
            
            "outputText": "Consider using AWS Backup for automated and centralized backup management of SAP HANA. \nImplement AWS Shield and AWS WAF for enhanced security against DDoS attacks. \nUse Amazon FSx for NetApp ONTAP for optimized file storage performance. \nEnable Amazon CloudWatch for real-time monitoring of SAP HANA performance metrics. \nUtilize AWS PrivateLink for secure and private connectivity between services. \nOptimize costs by implementing AWS Savings Plans for SAP HANA EC2 instances."
          
          

    },
    {
        "id":25,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.database import Dynamodb, RDS, Redshift\nfrom diagrams.aws.analytics import Kinesis\nfrom diagrams.aws.storage import S3\n\nwith Diagram(\"AWS Event-Driven Architecture\", show=False, direction=\"LR\"):\n    app_server_1 = Lambda(\"Application Server\")\n    app_server_2 = Lambda(\"Application Server\")\n    kinesis_stream = Kinesis(\"Source Events - Kinesis Streams\")\n\n    with Cluster(\"Event Processing\"):\n        app_updates = Lambda(\"Application Updates - Lambda\")\n        event_persistence = Lambda(\"Event Persistence - Lambda\")\n        analytics_updates = Lambda(\"Analytics Updates - Lambda\")\n\n    with Cluster(\"Data Storage\"):\n        ref_data = Dynamodb(\"Reference Data - DynamoDB\")\n        app_db = RDS(\"Application SQL Database - RDS\")\n        event_store = S3(\"Event Store - S3\")\n        analytics_db = Redshift(\"Analytics DB - Redshift\")\n\n    with Cluster(\"Event Collation\"):\n        event_collator = Lambda(\"Event Collator - Lambda\")\n\n    app_server_1 >> kinesis_stream\n    app_server_2 >> kinesis_stream\n    kinesis_stream >> [app_updates, event_persistence, analytics_updates]\n    app_updates >> [ref_data, app_db]\n    event_persistence >> event_store\n    analytics_updates >> analytics_db\n    event_store >> event_collator\n",
            
            "outputText": "Consider using AWS Step Functions for orchestrating event-driven workflows instead of multiple Lambda functions. \nUse Amazon Kinesis Data Firehose to directly stream and store data in Redshift for real-time analytics. \nEnable AWS Glue for automated ETL (Extract, Transform, Load) processes to improve data integration. \nImplement Amazon EventBridge instead of Kinesis Streams if a more scalable, managed event bus is required. \nUse AWS IAM policies to securely manage Lambda permissions and event sources. \nEnable Amazon CloudWatch Logs for better monitoring and debugging of event-driven processes."
          
          
    },
    {
        "id":26,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2, ElasticLoadBalancing\nfrom diagrams.aws.database import RDS, Dynamodb, Elasticache\nfrom diagrams.aws.network import Route53\nfrom diagrams.aws.storage import S3\nfrom diagrams.onprem.database import Mongodb, Cassandra\nfrom diagrams.onprem.inmemory import Redis, Memcached\n\nwith Diagram(\"AWS Web Application Architecture\", show=False, direction=\"TB\"):\n    route53 = Route53(\"Amazon Route 53\")\n    elb = ElasticLoadBalancing(\"Elastic Load Balancer\")\n    web_servers = EC2(\"Web Servers (Nginx/Apache)\")\n    app_servers = EC2(\"Application Servers (Backend)\")\n    s3 = S3(\"Amazon S3 Bucket\")\n\n    with Cluster(\"Databases & Caching\"):\n        mysql = RDS(\"MySQL\")\n        dynamodb = Dynamodb(\"Amazon DynamoDB for NoSQL\")\n        memcached = Memcached(\"Memcached Cache\")\n        redis = Redis(\"Redis Cache\")\n        mongodb = Mongodb(\"MongoDB\")\n        cassandra = Cassandra(\"Cassandra\")\n\n    route53 >> elb >> web_servers >> app_servers\n    app_servers >> [mysql, dynamodb, memcached, redis, mongodb, cassandra]\n    app_servers >> s3\n",
            
            "outputText": "Consider using AWS Auto Scaling to dynamically adjust the number of EC2 instances based on traffic demand. \nImplement Amazon CloudFront to enhance content delivery and reduce latency for end users. \nUse Amazon RDS Multi-AZ deployment for high availability and failover support for MySQL. \nEnable Amazon ElastiCache with Redis for faster in-memory caching and improved database performance. \nSecure web and application servers using AWS WAF to protect against common web exploits. \nImplement AWS CloudWatch for real-time monitoring and logging of web application performance."
                 
    },
    {
        "id":27,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2, AutoScaling\nfrom diagrams.aws.network import VPC, InternetGateway, LoadBalancer\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.general import User\n\nwith Diagram(\"AWS Three-Tier Architecture\", show=False, direction=\"TB\"):\n    user = User(\"End User\")\n    internet_gateway = InternetGateway(\"Internet Gateway\")\n    vpc = VPC(\"AWS VPC\")\n\n    with Cluster(\"Web Tier\"):\n        web_lb = LoadBalancer(\"Application Load Balancer\")\n        web_auto_scaling = AutoScaling(\"Auto Scaling Group\")\n        web_servers = [EC2(\"Web Server 1\"), EC2(\"Web Server 2\")]\n\n    with Cluster(\"Application Tier\"):\n        app_lb = LoadBalancer(\"Application Load Balancer\")\n        app_auto_scaling = AutoScaling(\"Auto Scaling Group\")\n        app_servers = [EC2(\"App Server 1\"), EC2(\"App Server 2\")]\n\n    with Cluster(\"Database Tier\"):\n        database = RDS(\"Primary Database\")\n        replica_db = RDS(\"Read Replica\")\n\n    user >> internet_gateway >> web_lb >> web_auto_scaling >> web_servers\n    web_servers >> app_lb >> app_auto_scaling >> app_servers\n    app_servers >> database\n    database >> replica_db\n",
            
            "outputText": "Consider using AWS Global Accelerator to improve the availability and performance of the web tier. \nImplement AWS WAF to protect against common web threats and attacks. \nUse Amazon RDS Multi-AZ deployment for automatic failover and high availability of the database tier. \nLeverage AWS Elastic Load Balancer (ELB) with Auto Scaling to dynamically adjust resources based on demand. \nEnable AWS CloudWatch for real-time monitoring of application performance and logs. \nUtilize AWS Secrets Manager to securely store database credentials and sensitive information."
          
          
    },
    {
        "id":28,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import ECS, EC2\nfrom diagrams.aws.network import VPC, InternetGateway, NATGateway, LoadBalancer\nfrom diagrams.aws.database import Dynamodb, RDS\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.security import WAF\n\nwith Diagram(\"AWS ECS Architecture\", show=False, direction=\"TB\"):\n    vpc = VPC(\"Virtual Private Cloud\")\n    internet_gateway = InternetGateway(\"Internet Gateway\")\n    nat_gateway_1 = NATGateway(\"NAT Gateway Zone 1\")\n    nat_gateway_2 = NATGateway(\"NAT Gateway Zone 2\")\n\n    with Cluster(\"Public Subnet\"):\n        bastion_host_1 = EC2(\"Bastion Host 1\")\n        bastion_host_2 = EC2(\"Bastion Host 2\")\n        load_balancer = LoadBalancer(\"Application Load Balancer\")\n\n    with Cluster(\"Private Subnet - ECS Cluster\"):\n        ecs_instances = [ECS(\"ECS Container Instance 1\"), ECS(\"ECS Container Instance 2\"), ECS(\"ECS Container Instance 3\")]\n\n    with Cluster(\"Storage & Databases\"):\n        rds = RDS(\"Amazon RDS\")\n        dynamodb = Dynamodb(\"Amazon DynamoDB\")\n        s3 = S3(\"Amazon S3 Bucket\")\n\n    internet_gateway >> load_balancer >> ecs_instances\n    nat_gateway_1 >> ecs_instances\n    nat_gateway_2 >> ecs_instances\n    bastion_host_1 >> nat_gateway_1\n    bastion_host_2 >> nat_gateway_2\n    ecs_instances >> [rds, dynamodb, s3]\n",
            
            "outputText": "Consider using AWS Fargate instead of ECS container instances to remove the need for managing EC2 infrastructure. \nImplement AWS Systems Manager for secure bastion-less access to ECS instances. \nUse AWS Auto Scaling for ECS tasks to automatically scale based on demand. \nEnable AWS Shield and AWS WAF for enhanced security against DDoS attacks. \nUse Amazon S3 lifecycle policies to optimize storage costs. \nImplement AWS CloudTrail for auditing and security monitoring of ECS clusters."
          
          
    },
    {
        "id":29,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2, AutoScaling\nfrom diagrams.aws.network import VPC, InternetGateway, LoadBalancer\nfrom diagrams.aws.database import RDS, Dynamodb\nfrom diagrams.aws.security import WAF\nfrom diagrams.aws.integration import SQS\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.aws.storage import S3\n\nwith Diagram(\"AWS OPA Architecture\", show=False, direction=\"TB\"):\n    vpc = VPC(\"AWS VPC\")\n    internet_gateway = InternetGateway(\"Internet Gateway\")\n    waf = WAF(\"AWS WAF\")\n\n    with Cluster(\"Public Subnet\"):\n        load_balancer = LoadBalancer(\"Application Load Balancer\")\n        auto_scaling = AutoScaling(\"Auto Scaling Group\")\n        web_servers = [EC2(\"Web Server 1\"), EC2(\"Web Server 2\"), EC2(\"Web Server 3\")]\n\n    with Cluster(\"Application Layer\"):\n        app_servers = [EC2(\"App Server 1\"), EC2(\"App Server 2\"), EC2(\"App Server 3\")]\n\n    with Cluster(\"Database Layer\"):\n        primary_db = RDS(\"Primary Database\")\n        replica_db = RDS(\"Read Replica\")\n        dynamodb = Dynamodb(\"DynamoDB\")\n\n    with Cluster(\"Storage & Messaging\"):\n        s3 = S3(\"Amazon S3\")\n        sqs = SQS(\"Amazon SQS\")\n        cloudwatch = Cloudwatch(\"AWS CloudWatch\")\n\n    internet_gateway >> waf >> load_balancer >> auto_scaling >> web_servers\n    web_servers >> app_servers\n    app_servers >> [primary_db, dynamodb, s3, sqs]\n    primary_db >> replica_db\n    sqs >> cloudwatch\n",
            
            "outputText": "Consider implementing AWS Global Accelerator to improve latency and availability across regions. \nUse AWS Shield Advanced for enhanced DDoS protection along with AWS WAF. \nEnable AWS Secrets Manager to securely manage database credentials. \nLeverage Amazon ElastiCache to optimize database performance and reduce query load. \nUse AWS Lambda for serverless functions to reduce infrastructure costs where applicable. \nImplement AWS Config to track changes and compliance in the architecture."
          
          
    },
    {
        "id":30,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2, AutoScaling\nfrom diagrams.aws.network import LoadBalancer, VPC\n\nwith Diagram(\"AWS EC2 Auto Scaling Architecture\", show=False, direction=\"TB\"):\n    vpc = VPC(\"AWS VPC\")\n    load_balancer = LoadBalancer(\"Elastic Load Balancer\")\n\n    with Cluster(\"Availability Zone 1\"):\n        auto_scaling_az1 = AutoScaling(\"EC2 Auto Scaling\")\n        ec2_instances_az1 = [EC2(\"EC2 Instance 1\"), EC2(\"EC2 Instance 2\")]\n\n    with Cluster(\"Availability Zone 2\"):\n        auto_scaling_az2 = AutoScaling(\"EC2 Auto Scaling\")\n        ec2_instances_az2 = [EC2(\"EC2 Instance 3\"), EC2(\"EC2 Instance 4\")]\n\n    load_balancer >> [auto_scaling_az1, auto_scaling_az2]\n    auto_scaling_az1 >> ec2_instances_az1\n    auto_scaling_az2 >> ec2_instances_az2\n",
            
            "outputText": "Use AWS Auto Scaling policies to dynamically adjust EC2 instances based on CPU utilization or request count. \nImplement AWS Elastic Load Balancer (ELB) with sticky sessions for optimized traffic distribution. \nEnable AWS CloudWatch metrics to monitor scaling activities and detect anomalies. \nUse AWS EC2 Spot Instances to optimize costs for non-critical workloads. \nEnable AWS Systems Manager for automated patching and maintenance of EC2 instances. \nConsider using AWS Lambda for serverless applications to reduce EC2 dependency."
          
          
    },
    {
        "id":31,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.database import Dynamodb\nfrom diagrams.aws.integration import SQS\nfrom diagrams.aws.network import APIGateway\nfrom diagrams.aws.devtools import Cloudformation\nfrom diagrams.aws.general import User\n\nwith Diagram(\"AWS Microservices Architecture\", show=False, direction=\"LR\"):\n    user = User(\"User\")\n    api_gateway = APIGateway(\"AWS API Gateway\")\n    cloudformation = Cloudformation(\"CloudFormation Stack\")\n\n    with Cluster(\"Product Microservice\"):\n        product_lambda = Lambda(\"AWS Lambda\")\n        product_db = Dynamodb(\"AWS DynamoDB\")\n\n    with Cluster(\"Basket Microservice\"):\n        basket_lambda = Lambda(\"AWS Lambda\")\n        basket_db = Dynamodb(\"AWS DynamoDB\")\n\n    with Cluster(\"Ordering Microservice\"):\n        ordering_lambda = Lambda(\"AWS Lambda\")\n        ordering_db = Dynamodb(\"AWS DynamoDB\")\n        sqs_queue = SQS(\"AWS SQS Queue\")\n\n    user >> api_gateway >> [product_lambda, basket_lambda, ordering_lambda]\n    product_lambda >> product_db\n    basket_lambda >> basket_db\n    ordering_lambda >> ordering_db\n    ordering_lambda >> sqs_queue\n",
            
            "outputText": "Consider using AWS Step Functions to orchestrate microservices workflows efficiently. \nEnable Amazon API Gateway caching to improve response times and reduce Lambda invocations. \nImplement AWS IAM roles to secure Lambda functions and ensure least privilege access. \nUse AWS X-Ray for distributed tracing and performance monitoring of microservices. \nImplement AWS EventBridge instead of SQS for more scalable event-driven communication. \nUtilize AWS WAF to protect API Gateway endpoints from common web threats."
          
          
    },
    {
        "id":32,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.database import Dynamodb\nfrom diagrams.aws.analytics import ElasticsearchService\nfrom diagrams.aws.integration import APIGateway\nfrom diagrams.aws.security import Cognito, IAM\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.management import Cloudwatch\n\nwith Diagram(\"AWS Data Lake Architecture\", show=False, direction=\"LR\"):\n    cognito = Cognito(\"Amazon Cognito\")\n    api_gateway = APIGateway(\"Amazon API Gateway (Data Lake RESTful API)\")\n    custom_auth_lambda = Lambda(\"AWS Lambda (Custom Authorizer)\")\n    data_lake_lambda = Lambda(\"AWS Lambda (Data Lake Microservices)\")\n\n    with Cluster(\"Data Storage & Logging\"):\n        s3 = S3(\"Amazon S3\")\n        dynamodb = Dynamodb(\"Amazon DynamoDB\")\n        es = ElasticsearchService(\"Amazon ES\")\n        cloudwatch = Cloudwatch(\"Amazon CloudWatch Logs\")\n        iam_roles = IAM(\"IAM Roles\")\n\n    cognito >> custom_auth_lambda >> api_gateway >> data_lake_lambda\n    data_lake_lambda >> [s3, dynamodb, es, cloudwatch, iam_roles]\n",
            
            "outputText": "Consider using AWS Glue for automated ETL (Extract, Transform, Load) processes to optimize data movement. \nImplement Amazon Kinesis for real-time data streaming into the Data Lake. \nEnable AWS CloudTrail for monitoring API requests and securing data access. \nUse AWS Lake Formation to simplify access management and security of data lake storage. \nLeverage Amazon OpenSearch (formerly Elasticsearch) for enhanced search and data indexing performance. \nOptimize S3 storage costs by utilizing Intelligent-Tiering for automatic data lifecycle management."
          
          
    },
    {
        "id":33,
    
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import Lambda, EC2\nfrom diagrams.aws.database import Dynamodb, RDS\nfrom diagrams.aws.analytics import Athena\nfrom diagrams.aws.integration import APIGateway, Eventbridge, SNS\nfrom diagrams.aws.ml import Glue\nfrom diagrams.aws.devtools import Codebuild\nfrom diagrams.aws.network import VPC, NATGateway, Route53\nfrom diagrams.aws.general import User\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.security import Cloudfront\nfrom diagrams.aws.monitoring import Cloudwatch\nfrom diagrams.onprem.vcs import Github\n\nwith Diagram(\"AWS Web Application Architecture\", show=False, direction=\"LR\"):\n    user = User(\"User\")\n    route53 = Route53(\"Amazon Route 53 Hosted Zone\")\n    cloudfront = Cloudfront(\"Amazon CloudFront\")\n    api_gateway = APIGateway(\"Amazon API Gateway\")\n    vpc = VPC(\"Virtual Private Cloud\")\n    public_subnet = NATGateway(\"Public Subnet\")\n    private_subnet = NATGateway(\"Private Subnet\")\n    load_balancer = EC2(\"Elastic Load Balancer\")\n\n    with Cluster(\"Web & App Tier\"):\n        web_server_1 = EC2(\"Web Server 1\")\n        web_server_2 = EC2(\"Web Server 2\")\n        app_server_1 = EC2(\"App Server 1\")\n        app_server_2 = EC2(\"App Server 2\")\n\n    with Cluster(\"Data Storage & Caching\"):\n        dynamodb = Dynamodb(\"Amazon DynamoDB\")\n        rds = RDS(\"Amazon RDS\")\n        datalake = S3(\"Amazon S3 Bucket\")\n        glue = Glue(\"AWS Glue\")\n        athena = Athena(\"Amazon Athena\")\n\n    with Cluster(\"CI/CD Integration\"):\n        github = Github(\"GitHub\")\n        codebuild = Codebuild(\"AWS CodeBuild\")\n\n    with Cluster(\"Monitoring & Notifications\"):\n        eventbridge = Eventbridge(\"Amazon EventBridge\")\n        sns = SNS(\"Amazon SNS\")\n        cloudwatch = Cloudwatch(\"Amazon CloudWatch Alarms\")\n\n    user >> route53 >> cloudfront >> load_balancer >> [web_server_1, web_server_2]\n    web_server_1 >> app_server_1\n    web_server_2 >> app_server_2\n    app_server_1 >> [rds, dynamodb]\n    app_server_2 >> [rds, dynamodb]\n    app_server_1 >> datalake >> glue >> athena\n    api_gateway >> lambda_function\n    lambda_function >> [eventbridge, sns]\n    github >> codebuild >> lambda_function\n",
            
            "outputText": "Consider enabling AWS Auto Scaling for EC2 instances to optimize resource allocation and reduce costs. \nImplement Amazon CloudFront caching to enhance performance and reduce latency. \nUse Amazon ElastiCache for Redis to optimize database query performance and reduce load on RDS. \nLeverage AWS WAF to protect against web application vulnerabilities and attacks. \nEnable AWS CloudWatch monitoring for real-time performance tracking and automated alerts. \nUse AWS Secrets Manager to securely store database credentials and sensitive application configurations."
          
          
    },
    {
        "id":34,

            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.database import Dynamodb\nfrom diagrams.aws.analytics import Athena\nfrom diagrams.aws.integration import APIGateway, Eventbridge, SNS\nfrom diagrams.aws.ml import Glue\nfrom diagrams.aws.devtools import Codebuild\nfrom diagrams.aws.network import VPC, NATGateway\nfrom diagrams.aws.general import User\nfrom diagrams.aws.storage import S3\nfrom diagrams.onprem.vcs import Github\n\nwith Diagram(\"AWS Serverless Data Pipeline\", show=False, direction=\"LR\"):\n    user = User(\"User\")\n    api_gateway = APIGateway(\"Amazon API Gateway\")\n    vpc = VPC(\"Virtual Private Cloud\")\n    public_subnet = NATGateway(\"Public Subnet\")\n    private_subnet = NATGateway(\"Private Subnet\")\n    lambda_function = Lambda(\"Lambda Function (Python Boto)\")\n\n    with Cluster(\"Data Processing\"):\n        dynamodb = Dynamodb(\"Amazon DynamoDB\")\n        datalake = S3(\"Datalake (S3)\")\n        glue = Glue(\"AWS Glue\")\n        athena = Athena(\"Amazon Athena\")\n\n    with Cluster(\"CI/CD Integration\"):\n        github = Github(\"GitHub\")\n        codebuild = Codebuild(\"AWS CodeBuild\")\n\n    with Cluster(\"Event Triggering & Notifications\"):\n        eventbridge = Eventbridge(\"Amazon EventBridge\")\n        sns = SNS(\"Amazon SNS\")\n        document_storage = S3(\"Document Storage (S3)\")\n\n    user >> api_gateway >> lambda_function\n    lambda_function >> [dynamodb, eventbridge]\n    eventbridge >> dynamodb >> datalake >> glue >> athena\n    lambda_function >> sns >> document_storage\n    github >> codebuild >> lambda_function\n",
            
            "outputText": "Consider enabling AWS Step Functions to orchestrate Lambda functions for better modularization. \nUse AWS Secrets Manager to securely store credentials used by Lambda and database connections. \nImplement AWS CloudTrail for auditing API Gateway requests and security monitoring. \nOptimize S3 storage with lifecycle policies to automatically archive or delete old data. \nEnable AWS Config to track changes in the infrastructure for compliance and security. \nUse Amazon Kinesis for real-time data ingestion instead of relying on batch processing."
          
          
    },
    {
        "id":35,
        "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.database import Dynamodb, RDS\nfrom diagrams.aws.network import VPC, InternetGateway, LoadBalancer\nfrom diagrams.aws.security import IAM\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.management import Route53\nfrom diagrams.aws.integration import SNS, SQS\n\nwith Diagram(\"AWS VPC Architecture\", show=False, direction=\"TB\"):\n    vpc = VPC(\"AWS VPC\")\n    internet_gateway = InternetGateway(\"Internet Gateway\")\n    load_balancer = LoadBalancer(\"Elastic Load Balancing\")\n\n    with Cluster(\"Availability Zone A\"):\n        with Cluster(\"Subnet (App 1)\"):\n            app_server_1 = EC2(\"EC2 Instance\")\n        with Cluster(\"Subnet (Data 1)\"):\n            database_1 = Dynamodb(\"Amazon DynamoDB\")\n            rds_1 = RDS(\"Amazon RDS\")\n\n    with Cluster(\"Availability Zone B\"):\n        with Cluster(\"Subnet (App 2)\"):\n            app_server_2 = EC2(\"EC2 Instance\")\n        with Cluster(\"Subnet (Data 2)\"):\n            database_2 = Dynamodb(\"Amazon DynamoDB\")\n            rds_2 = RDS(\"Amazon RDS\")\n\n    route53 = Route53(\"Route 53\")\n    s3 = S3(\"Amazon S3\")\n    iam = IAM(\"IAM\")\n    ses = SNS(\"Amazon SES\")\n    sns = SNS(\"Amazon SNS\")\n    sqs = SQS(\"Amazon SQS\")\n\n    internet_gateway >> load_balancer >> [app_server_1, app_server_2]\n    app_server_1 >> [database_1, rds_1]\n    app_server_2 >> [database_2, rds_2]\n    [route53, s3, iam, ses, sns, sqs] >> vpc\n",
            
        "outputText": "Enable AWS Auto Scaling to dynamically adjust EC2 instances for better resource utilization. \nImplement AWS CloudTrail for logging API calls and security auditing within the VPC. \nUse AWS WAF to protect Load Balancer endpoints from common threats. \nLeverage AWS Security Groups and Network ACLs to enforce strict access controls. \nEnable Multi-AZ deployment for RDS to ensure high availability and fault tolerance. \nUse Amazon S3 lifecycle policies to automatically manage data storage costs."
          
    },
    {
        "id":36,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.network import VPC, InternetGateway, LoadBalancer, VPNTunnel\nfrom diagrams.aws.security import IAM\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.management import Route53\nfrom diagrams.aws.integration import Cloudfront\nfrom diagrams.onprem.database import Oracle\nfrom diagrams.onprem.network import Internet\nfrom diagrams.onprem.client import Users\n\nwith Diagram(\"AWS Hybrid Cloud Architecture\", show=False, direction=\"TB\"):\n    users = Users(\"Users\")\n    internet = Internet(\"Internet\")\n    route53 = Route53(\"Route 53\")\n    cloudfront = Cloudfront(\"Amazon CloudFront\")\n    vpc = VPC(\"AWS VPC\")\n    internet_gateway = InternetGateway(\"Internet Gateway\")\n    vpn_gateway = VPNTunnel(\"VPN Gateway\")\n    load_balancer = LoadBalancer(\"Application Load Balancer\")\n\n    with Cluster(\"Availability Zone A\"):\n        ec2_instance_a = EC2(\"EC2 Instance\")\n        oracle_db_a = Oracle(\"Oracle DB\")\n        rds_a = RDS(\"Amazon RDS\")\n\n    with Cluster(\"Availability Zone B\"):\n        ec2_instance_b = EC2(\"EC2 Instance\")\n        oracle_db_b = Oracle(\"Oracle DB\")\n        rds_b = RDS(\"Amazon RDS\")\n\n    users >> internet >> route53 >> cloudfront >> load_balancer >> [ec2_instance_a, ec2_instance_b]\n    ec2_instance_a >> [oracle_db_a, rds_a]\n    ec2_instance_b >> [oracle_db_b, rds_b]\n    vpn_gateway >> [oracle_db_a, oracle_db_b]\n",
            
            "outputText": "Implement AWS Direct Connect for a dedicated network link between on-premises and AWS for lower latency. \nEnable AWS Shield for enhanced protection against DDoS attacks on CloudFront and ALB. \nUse AWS Transit Gateway to simplify VPC connectivity across hybrid environments. \nLeverage Amazon RDS Multi-AZ deployments to improve database availability and failover support. \nEnable AWS WAF to filter and protect against malicious web traffic targeting CloudFront and Load Balancer. \nUse AWS Systems Manager to automate patching and monitoring of EC2 instances."
          
          
    },
    {
        "id":37,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.database import Dynamodb\nfrom diagrams.aws.analytics import Kinesis\nfrom diagrams.aws.ml import ElasticCache\nfrom diagrams.aws.compute import ECS\nfrom diagrams.aws.integration import ApplicationLoadBalancer\nfrom diagrams.aws.devtools import ECR\nfrom diagrams.onprem.client import Users\n\nwith Diagram(\"AWS Streaming Data Processing\", show=False, direction=\"LR\"):\n    user = Users(\"Data Ingestion\")\n    ecr = ECR(\"Amazon ECR\")\n    ecs = ECS(\"Amazon ECS\")\n    alb = ApplicationLoadBalancer(\"Application Load Balancer\")\n    kinesis_stream = Kinesis(\"Kinesis Stream\")\n    lambda_function = Lambda(\"Lambda Function\")\n    dynamodb = Dynamodb(\"Amazon DynamoDB\")\n\n    with Cluster(\"Data Updates\"):\n        elasticache = ElasticCache(\"ElastiCache Cluster\")\n        lambda_update = Lambda(\"Lambda Function\")\n        kinesis_update = Kinesis(\"Kinesis Stream\")\n        core_data_update = Users(\"Core Data Update\")\n\n    user >> alb >> ecs >> kinesis_stream >> lambda_function >> dynamodb\n    ecs >> ecr\n    elasticache >> lambda_update >> kinesis_update >> core_data_update\n",
            
            "outputText": "Use Amazon Kinesis Data Firehose to simplify data streaming and automatic data transformation. \nImplement AWS Lambda concurrency controls to optimize function scaling and prevent throttling. \nLeverage Amazon DynamoDB auto-scaling to handle fluctuations in data writes efficiently. \nEnable AWS CloudWatch for real-time monitoring of streaming data and Lambda execution. \nUtilize Amazon ElastiCache with Redis to optimize real-time data processing and reduce database load. \nConsider using Amazon MSK (Managed Streaming for Apache Kafka) for more complex data streaming needs."
          
          
    },
    {
        "id":38,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.network import VPC, InternetGateway\nfrom diagrams.onprem.client import Users\n\nwith Diagram(\"AWS VPC EC2 Architecture\", show=False, direction=\"LR\"):\n    users = Users(\"Users\")\n    vpc = VPC(\"AWS VPC\")\n    internet_gateway = InternetGateway(\"Internet Gateway\")\n    ec2_instance = EC2(\"EC2 Instance\")\n\n    users >> internet_gateway >> ec2_instance\n",
            
            "outputText": "Enable AWS Security Groups and Network ACLs to restrict inbound and outbound traffic. \nImplement AWS Auto Scaling to ensure high availability and performance during traffic spikes. \nUse AWS CloudWatch for monitoring EC2 instance performance and setting up alerts. \nEnable AWS IAM roles for EC2 instances to follow the principle of least privilege. \nConsider using AWS Systems Manager for patch management and automation. \nUse AWS Elastic Load Balancer (ELB) to distribute incoming traffic across multiple EC2 instances."
          
          
    },
    {
        "id":39,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2, Lambda, ECS, EKS\nfrom diagrams.aws.database import Dynamodb, Aurora\nfrom diagrams.aws.network import Cloudwatch\nfrom diagrams.aws.integration import SQS, MSK\nfrom diagrams.aws.storage import S3\nfrom diagrams.onprem.database import IBMDb2\nfrom diagrams.onprem.compute import MQ\nfrom diagrams.onprem.client import Users\n\nwith Diagram(\"IBM Instana SaaS with AWS Hybrid Architecture\", show=False, direction=\"LR\"):\n    users = Users(\"Developers and SREs\")\n    corporate_data_center = Cluster(\"Corporate Data Center\")\n    instana_agents = Cluster(\"Instana Agents\")\n    stream_processor = Cluster(\"IBM Instana SaaS\")\n    customer_env = Cluster(\"Customer Environment\")\n\n    with corporate_data_center:\n        servers = Users(\"Servers\")\n        applications = Users(\"Applications\")\n        databases = Users(\"Databases\")\n\n    with stream_processor:\n        ui_backend = Users(\"UI Backend\")\n        sensor_repository = Users(\"Sensor Repository\")\n\n    with customer_env:\n        with Cluster(\"Instana Agents\"):\n            ec2 = EC2(\"Amazon EC2\")\n            eks = EKS(\"Amazon EKS\")\n            ecs = ECS(\"Amazon ECS\")\n            rosa = Users(\"Red Hat OpenShift on AWS (ROSA)\")\n\n        cloudwatch = Cloudwatch(\"Amazon CloudWatch\")\n\n        with Cluster(\"AWS Services\"):\n            lambda_func = Lambda(\"AWS Lambda\")\n            dynamodb = Dynamodb(\"Amazon DynamoDB\")\n            s3 = S3(\"Amazon S3\")\n            sqs = SQS(\"Amazon SQS\")\n            aurora = Aurora(\"Amazon Aurora\")\n            msk = MSK(\"Amazon MSK\")\n            ibm_mq = MQ(\"IBM MQ\")\n            ibm_db2 = IBMDb2(\"IBM Db2\")\n\n    users >> stream_processor >> sensor_repository\n    sensor_repository >> corporate_data_center\n    corporate_data_center >> instana_agents >> customer_env\n    customer_env >> cloudwatch\n    instana_agents >> [ec2, eks, ecs, rosa]\n    cloudwatch >> [lambda_func, dynamodb, s3, sqs, aurora, msk, ibm_mq, ibm_db2]\n",
            
            "outputText": "Use Amazon EventBridge for event-driven architecture to reduce dependency on direct integrations. \nLeverage AWS Identity and Access Management (IAM) to enforce least privilege access to services. \nImplement AWS CloudTrail to monitor and log all API activity across AWS services. \nOptimize data flow between IBM Instana SaaS and AWS services using AWS PrivateLink. \nEnable Amazon GuardDuty to enhance security and threat detection in customer environments. \nConsider using Amazon FSx for shared file systems instead of self-managed solutions."
          
          
    },
    {
        "id":40,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2, ECS\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.ml import ElasticCache\nfrom diagrams.aws.network import VPC, InternetGateway, LoadBalancer\nfrom diagrams.aws.security import IAM, WAF, CloudTrail\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.aws.integration import SNS\nfrom diagrams.aws.network import CloudFront\n\nwith Diagram(\"AWS Secure Web Application Architecture\", show=False, direction=\"LR\"):\n    cloudfront = CloudFront(\"Amazon CloudFront\")\n    waf = WAF(\"AWS WAF\")\n    s3 = S3(\"Amazon S3\")\n    internet_gateway = InternetGateway(\"Internet Gateway\")\n    load_balancer = LoadBalancer(\"Application Load Balancer\")\n    vpc = VPC(\"AWS VPC\")\n\n    with Cluster(\"VPC\"):\n        ecs = ECS(\"ECS\")\n        asg = EC2(\"Auto Scaling Group\")\n        ec2_instance_1 = EC2(\"EC2 Instance\")\n        ec2_instance_2 = EC2(\"EC2 Instance\")\n        rds = RDS(\"Amazon RDS\")\n        elasticache = ElasticCache(\"Amazon ElastiCache\")\n\n    with Cluster(\"Security & Monitoring\"):\n        iam = IAM(\"AWS Identity & Access Management\")\n        sns = SNS(\"Amazon SNS\")\n        cloudwatch = Cloudwatch(\"Amazon CloudWatch\")\n        cloudtrail = CloudTrail(\"AWS CloudTrail\")\n\n    cloudfront >> waf >> s3 >> internet_gateway >> load_balancer >> ecs\n    ecs >> asg >> [ec2_instance_1, ec2_instance_2]\n    ec2_instance_1 >> [rds, elasticache]\n    ec2_instance_2 >> [rds, elasticache]\n    [iam, sns, cloudwatch, cloudtrail] >> vpc\n",
            
            "outputText": "Enable AWS Shield Advanced for enhanced DDoS protection beyond AWS WAF. \nImplement AWS Config to continuously assess the compliance of security configurations. \nUse AWS Secrets Manager to securely store and rotate database credentials. \nEnable Amazon GuardDuty to detect suspicious activity and potential security threats. \nLeverage AWS Systems Manager to automate patch management and operational monitoring. \nUtilize AWS Certificate Manager (ACM) to manage SSL/TLS certificates for CloudFront."
          
          
    },
    {
        "id":41,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.ml import ElasticCache\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.network import LoadBalancer, VPC\nfrom diagrams.aws.storage import S3\nfrom diagrams.onprem.client import Users\n\nwith Diagram(\"AWS Web & Game App with ElastiCache\", show=False, direction=\"TB\"):\n    users = Users(\"Users\")\n    cdn = S3(\"CDN\")\n    assets_bucket = S3(\"Assets Bucket\")\n    vpc = VPC(\"AWS VPC\")\n    load_balancer = LoadBalancer(\"Application Load Balancer\")\n\n    with Cluster(\"Web Application Servers\"):\n        web_app_servers = [EC2(\"Web App Server\") for _ in range(3)]\n        web_master_db = RDS(\"Web App Master DB\")\n        web_replica_db = RDS(\"Web App Replica DB\")\n\n    with Cluster(\"Game Application Servers\"):\n        game_app_servers = [EC2(\"Game App Server\") for _ in range(3)]\n        game_master_db = RDS(\"Game App Master DB\")\n        game_replica_db = RDS(\"Game App Replica DB\")\n\n    with Cluster(\"ElastiCache Cluster\"):\n        redis_cluster = ElasticCache(\"Redis Nodes\")\n\n    users >> cdn >> load_balancer >> web_app_servers\n    users >> cdn >> load_balancer >> game_app_servers\n    assets_bucket >> load_balancer\n    web_app_servers >> web_master_db >> web_replica_db\n    game_app_servers >> game_master_db >> game_replica_db\n    redis_cluster >> web_app_servers\n    redis_cluster >> game_app_servers\n",
            
            "outputText": "Enable Amazon CloudFront caching to reduce latency for content delivery. \nUse Amazon Aurora instead of RDS for better performance and scalability in the databases. \nImplement AWS Auto Scaling for EC2 instances to handle variable workloads efficiently. \nEnable AWS Security Groups to enforce strict access controls on web and game servers. \nLeverage AWS Systems Manager to automate patch management and monitoring. \nUse Redis Cluster mode to scale ElastiCache horizontally for better performance."
          
          
    },
    {
        "id":42,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.database import Dynamodb\nfrom diagrams.aws.network import VPC, InternetGateway, NATGateway, RouteTable, VPNGateway\nfrom diagrams.aws.storage import S3\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.network import Firewall\n\nwith Diagram(\"AWS VPC Network Architecture\", show=False, direction=\"TB\"):\n    internet = InternetGateway(\"Internet Gateway\")\n    vpc = VPC(\"VPC 10.0.0.0/16\")\n\n    with Cluster(\"Public VPC Subnet 10.0.0.0/24\"):\n        nat_gateway = NATGateway(\"NAT Gateway\\n10.0.1.31\")\n        instance1 = EC2(\"Instance 10.0.1.10\")\n        route_table_pub = RouteTable(\"Route Table\")\n\n    with Cluster(\"Private VPC Subnet 10.0.1.0/24\"):\n        instance2 = EC2(\"Instance 10.0.1.11\")\n        instance3 = EC2(\"Instance 10.0.1.12\")\n        route_table_priv = RouteTable(\"Route Table\")\n\n    dynamo = Dynamodb(\"Amazon DynamoDB\")\n    s3 = S3(\"S3 Bucket\")\n    vpn_gateway = VPNGateway(\"VPN Gateway\")\n    corporate_data_center = Users(\"Corporate Data Center\")\n    firewall = Firewall(\"Security Firewall\")\n\n    internet >> nat_gateway >> instance1\n    instance1 >> route_table_pub\n    nat_gateway >> instance2\n    nat_gateway >> instance3\n    instance2 >> route_table_priv\n    instance3 >> route_table_priv\n    instance1 >> Edge(label=\"Data Flow\") >> dynamo\n    instance2 >> Edge(label=\"Data Storage\") >> s3\n    vpn_gateway >> corporate_data_center\n    route_table_priv >> vpn_gateway\n    route_table_pub >> internet\n    firewall >> vpc\n",
            
            "outputText": "Enable AWS Security Groups and Network ACLs to enforce strict access control rules. \nImplement AWS Auto Scaling for EC2 instances to manage workloads efficiently. \nUse AWS CloudTrail to monitor API activity and detect security threats. \nLeverage AWS Transit Gateway for simplified network management and connectivity. \nEnable AWS Config to track changes in network configurations for compliance auditing. \nUse AWS Direct Connect for a more stable and secure connection to the corporate data center."
          
          
    },
    {
        "id":43,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.ml import ElasticCache\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.network import LoadBalancer, VPC\nfrom diagrams.aws.storage import S3, EBS\nfrom diagrams.aws.network import CloudFront\nfrom diagrams.onprem.client import Users\n\nwith Diagram(\"AWS Web Application Auto Scaling\", show=False, direction=\"TB\"):\n    users = Users(\"Users\")\n    cloudfront = CloudFront(\"CloudFront Distribution\")\n    s3_bucket = S3(\"Amazon S3 Bucket\")\n    load_balancer = LoadBalancer(\"Application Load Balancer\")\n    vpc = VPC(\"AWS VPC\")\n\n    with Cluster(\"Auto Scaling Group\"):\n        web_server = EC2(\"Web App Server\")\n        root_volume = EBS(\"Root Volume\")\n        data_volume = EBS(\"Data Volume\")\n        ebs_snapshot = S3(\"Amazon EBS Snapshot\")\n\n    with Cluster(\"Database Layer\"):\n        database = RDS(\"RDS Database\")\n        security_group = ElasticCache(\"Security Group\")\n\n    users >> cloudfront >> s3_bucket\n    users >> load_balancer >> web_server\n    web_server >> [root_volume, data_volume, ebs_snapshot]\n    web_server >> database\n    database >> security_group\n",
            
            "outputText": "Enable AWS Auto Scaling policies to automatically adjust the number of EC2 instances based on traffic. \nUse Amazon CloudWatch to monitor system performance and set up alerts for resource utilization. \nImplement AWS Backup to automate EBS snapshot management for disaster recovery. \nLeverage AWS Identity and Access Management (IAM) to control and audit access to critical resources. \nOptimize Amazon S3 lifecycle policies to move older logs to Glacier for cost savings. \nEnable AWS Shield Standard to protect CloudFront distribution from common DDoS attacks."
          
          
    },
    {
        "id":44,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.database import Dynamodb, RDS\nfrom diagrams.aws.network import VPC, InternetGateway, LoadBalancer\nfrom diagrams.aws.security import IAM\nfrom diagrams.aws.storage import S3\nfrom diagrams.onprem.network import Firewall\n\nwith Diagram(\"AWS Architecture Diagram\", show=False, direction=\"TB\"):\n    vpc = VPC(\"AWS VPC\")\n    internet_gateway = InternetGateway(\"Internet Gateway\")\n    load_balancer = LoadBalancer(\"Elastic Load Balancer\")\n    firewall = Firewall(\"Web App Firewall\")\n\n    with Cluster(\"Public Subnet\"):\n        web_server1 = EC2(\"Web App Server 1\")\n        web_server2 = EC2(\"Web App Server 2\")\n\n    with Cluster(\"Private Subnet\"):\n        app_server1 = EC2(\"Application Server 1\")\n        app_server2 = EC2(\"Application Server 2\")\n\n    with Cluster(\"Database Layer\"):\n        mysql_db = RDS(\"MySQL Database\")\n        dynamodb = Dynamodb(\"Amazon DynamoDB\")\n        storage = S3(\"Amazon S3\")\n        security_group = IAM(\"Security Group\")\n\n    internet_gateway >> firewall >> load_balancer >> [web_server1, web_server2]\n    web_server1 >> app_server1\n    web_server2 >> app_server2\n    app_server1 >> mysql_db\n    app_server2 >> dynamodb\n    [mysql_db, dynamodb] >> storage\n    security_group >> vpc\n",
            
            "outputText": "Enable AWS WAF to provide additional security for the web application firewall. \nImplement AWS Auto Scaling to dynamically adjust the number of EC2 instances based on demand. \nUse Amazon CloudFront to improve performance by caching static content closer to users. \nEnable Amazon RDS Multi-AZ for high availability and failover support in database instances. \nUtilize AWS Secrets Manager to securely store and rotate database credentials. \nMonitor logs and system performance using Amazon CloudWatch for better visibility."
          
          
    },
    {
        "id":45,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import ECS\nfrom diagrams.aws.database import Aurora, Dynamodb\nfrom diagrams.aws.ml import ElasticCache\nfrom diagrams.aws.network import LoadBalancer\nfrom diagrams.aws.network import CloudFront\nfrom diagrams.aws.storage import S3\n\nwith Diagram(\"AWS Microservices Architecture\", show=False, direction=\"LR\"):\n    cloudfront = CloudFront(\"Amazon CloudFront\")\n    s3 = S3(\"Amazon S3\")\n    alb = LoadBalancer(\"ALB\")\n    ecs = ECS(\"Amazon ECS\")\n\n    with Cluster(\"Data Store\"):\n        elasticache = ElasticCache(\"Amazon ElastiCache\")\n        aurora = Aurora(\"Amazon Aurora\")\n        dynamodb = Dynamodb(\"Amazon DynamoDB\")\n\n    cloudfront >> s3 >> alb >> ecs\n    ecs >> [elasticache, aurora, dynamodb]\n",
            
            "outputText": "Enable AWS Auto Scaling for ECS to dynamically adjust service capacity. \nUse Amazon API Gateway to manage and secure API endpoints efficiently. \nImplement AWS Secrets Manager to securely store and rotate database credentials. \nLeverage AWS CloudWatch for ECS monitoring and setting up alerts for system health. \nEnable AWS WAF on CloudFront to protect against common web threats. \nOptimize database performance by using Amazon Aurora's read replicas."
          
          
    },
    {
        "id":46,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.database import Dynamodb, RDS\nfrom diagrams.aws.analytics import Kinesis, Redshift\nfrom diagrams.aws.storage import S3\nfrom diagrams.onprem.client import Users\n\nwith Diagram(\"AWS Kinesis Stream Processing\", show=False, direction=\"LR\"):\n    application_server1 = Users(\"Application Server\")\n    application_server2 = Users(\"Application Server\")\n    kinesis_stream = Kinesis(\"Source Events - Kinesis Streams\")\n\n    with Cluster(\"Data Processing\"):\n        application_lambda = Lambda(\"Application Updates - Lambda\")\n        persistence_lambda = Lambda(\"Event Persistence - Lambda\")\n        analytics_lambda = Lambda(\"Analytics Updates - Lambda\")\n\n    with Cluster(\"Data Storage\"):\n        dynamodb = Dynamodb(\"Reference Data - DynamoDB\")\n        rds = RDS(\"Application SQL Database - RDS\")\n        event_store = S3(\"Event Store - S3\")\n        redshift = Redshift(\"Analytics DB - Redshift\")\n\n    application_server1 >> kinesis_stream\n    application_server2 >> kinesis_stream\n    kinesis_stream >> application_lambda >> [dynamodb, rds]\n    kinesis_stream >> persistence_lambda >> event_store\n    kinesis_stream >> analytics_lambda >> redshift\n",
            
            "outputText": "Enable AWS Auto Scaling for Lambda functions to handle increased data flow efficiently. \nUse Amazon CloudWatch to monitor Kinesis Stream and Lambda execution performance. \nImplement AWS Glue for ETL processes instead of Lambda for large-scale data transformation. \nLeverage Amazon S3 lifecycle policies to archive older event data into Glacier for cost savings. \nUtilize AWS IAM roles to enforce least privilege access to data storage and processing layers. \nEnable Kinesis Data Firehose for automated data delivery to S3, Redshift, and other destinations."
          
          
    },
    {
        "id":47,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.ml import ElasticCache\nfrom diagrams.aws.network import LoadBalancer, Route53\nfrom diagrams.aws.security import IAM\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.network import CloudFront\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.media import Wowza\n\nwith Diagram(\"AWS Streaming Architecture\", show=False, direction=\"LR\"):\n    users = [Users(\"Web Client\") for _ in range(3)]\n    ip_tv = Users(\"IP TV\")\n    television = Users(\"Television\")\n    cloudfront = CloudFront(\"Amazon CloudFront\")\n    streaming_distribution = ElasticCache(\"Streaming Distribution\")\n    wowza = Wowza(\"Wowza Streaming Server\")\n\n    with Cluster(\"AWS Services\"):\n        route53 = Route53(\"Amazon Route 53\")\n        load_balancer = LoadBalancer(\"Elastic Load Balancer\")\n        auto_scaling = EC2(\"Auto-Scaling Group\")\n        instances = EC2(\"Instances\")\n        s3 = S3(\"Amazon S3\")\n        rds = RDS(\"Amazon RDS\")\n\n    for user in users:\n        user >> route53\n    route53 >> load_balancer >> auto_scaling >> instances\n    instances >> s3\n    instances >> rds\n    instances >> streaming_distribution >> cloudfront\n    cloudfront >> [wowza, ip_tv, television]\n",
            
            "outputText": "Enable AWS Auto Scaling for EC2 instances to handle peak streaming loads efficiently. \nUse Amazon CloudWatch for real-time monitoring of streaming performance and instance health. \nImplement AWS Shield Standard to protect CloudFront and streaming servers from DDoS attacks. \nLeverage AWS Elemental MediaLive for better live video processing instead of self-hosted Wowza servers. \nConfigure Amazon S3 lifecycle policies to archive older streaming assets to Amazon Glacier. \nUse Amazon Cognito for user authentication and access control in streaming applications."
          
          
    },
    {
        "id":48,
      
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import ECS, Fargate, Lambda\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.integration import MQ, SNS\nfrom diagrams.aws.network import APIGateway, LoadBalancer\nfrom diagrams.aws.storage import S3\nfrom diagrams.aws.management import Cloudwatch\nfrom diagrams.onprem.client import Users\n\nwith Diagram(\"AWS Messaging and Microservices Architecture\", show=False, direction=\"LR\"):\n    users = Users(\"On-Premises Clients\")\n    transfer_family = S3(\"AWS Transfer Family\")\n    s3 = S3(\"Amazon S3\")\n    api_gateway = APIGateway(\"Amazon API Gateway\")\n    load_balancer = LoadBalancer(\"Elastic Load Balancer\")\n\n    with Cluster(\"AWS Cloud\"):\n        fargate_task1 = Fargate(\"AWS Fargate\")\n        fargate_task2 = Fargate(\"AWS Fargate\")\n        fargate_task3 = Fargate(\"AWS Fargate\")\n        messaging_queue = MQ(\"Amazon MQ\")\n\n    with Cluster(\"Scaling Mechanism\"):\n        cloudwatch = Cloudwatch(\"Amazon CloudWatch\")\n        sns = SNS(\"Amazon SNS\")\n        lambda_function = Lambda(\"AWS Lambda\")\n\n    with Cluster(\"Microservices Layer\"):\n        ecs = ECS(\"Amazon ECS\")\n        database = RDS(\"Amazon RDS\")\n\n    users >> transfer_family >> s3\n    users >> api_gateway >> load_balancer >> ecs\n    ecs >> database\n    fargate_task1 >> messaging_queue\n    fargate_task2 >> messaging_queue\n    fargate_task3 >> messaging_queue\n    messaging_queue >> [cloudwatch, sns, lambda_function]\n",
            
            "outputText": "Enable AWS Auto Scaling for ECS and Fargate tasks to manage workload changes efficiently. \nUse Amazon CloudWatch for real-time monitoring of microservices and messaging queue performance. \nImplement AWS Secrets Manager to securely store and manage credentials for RDS and messaging queues. \nLeverage AWS Step Functions for better orchestration of microservices interactions. \nEnable AWS WAF to add security layers to API Gateway and prevent unauthorized access. \nOptimize Amazon S3 lifecycle policies to automatically archive older data to Amazon Glacier."
          
          
    },
    {
        "id":49,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.network import CloudFront\nfrom diagrams.aws.storage import S3\nfrom diagrams.onprem.client import Users\n\nwith Diagram(\"AWS CloudFront Distribution\", show=False, direction=\"LR\"):\n    source_s3 = S3(\"Amazon S3 DataStore\")\n    source_elemental1 = S3(\"AWS Elemental MediaPackage\")\n    source_elemental2 = S3(\"AWS Elemental MediaStore\")\n    cloudfront = CloudFront(\"Amazon CloudFront\")\n\n    with Cluster(\"Regional Edge Caches (REC)\"):\n        edge_cache_1 = CloudFront(\"Regional Edge Cache (REC)\")\n        edge_cache_2 = CloudFront(\"Regional Edge Cache (REC)\")\n\n    with Cluster(\"Edge Locations\"):\n        edge_location_1 = CloudFront(\"Edge Location\")\n        edge_location_2 = CloudFront(\"Edge Location\")\n        edge_location_3 = CloudFront(\"Edge Location\")\n        edge_location_4 = CloudFront(\"Edge Location\")\n\n    viewers = [Users(\"Viewer\") for _ in range(6)]\n\n    [source_s3, source_elemental1, source_elemental2] >> cloudfront\n    cloudfront >> [edge_cache_1, edge_cache_2]\n    edge_cache_1 >> [edge_location_1, edge_location_2]\n    edge_cache_2 >> [edge_location_3, edge_location_4]\n    edge_location_1 >> viewers[:2]\n    edge_location_2 >> viewers[2:4]\n    edge_location_3 >> viewers[4:5]\n    edge_location_4 >> viewers[5:]\n",
            
            "outputText": "Enable AWS Shield Standard for CloudFront to mitigate common DDoS attacks. \nUse AWS WAF to apply security rules at CloudFront edge locations to block malicious requests. \nOptimize caching policies in CloudFront to reduce latency and minimize data transfer costs. \nEnable CloudFront Logging to monitor performance and troubleshoot potential issues. \nUse Lambda@Edge for executing logic closer to users to improve application responsiveness. \nImplement Origin Shield to further optimize cache hit ratios and reduce load on the origin."
          
          
    },
    {
        "id":50,
        
            "inputCode": "from diagrams import Diagram, Cluster, Edge\nfrom diagrams.aws.compute import Lambda\nfrom diagrams.aws.integration import SQS, SNS\nfrom diagrams.aws.network import APIGateway\nfrom diagrams.aws.ml import Comprehend, Transcribe\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.iac import Ansible\n\nwith Diagram(\"AWS Support Ticketing System\", show=False, direction=\"LR\"):\n    users = Users(\"Users\")\n    support = Users(\"Support\")\n    api_gateway = APIGateway(\"Amazon API Gateway\")\n    sqs = SQS(\"Amazon SQS\")\n    lambda_function = Lambda(\"SQS Invoke Function\")\n\n    with Cluster(\"Processing Pipeline\"):\n        message_validity = Lambda(\"Message Validity Function\")\n        transcribe_function = Lambda(\"Transcribe Function\")\n        process_function = Lambda(\"Process Function\")\n\n    with Cluster(\"AI & Reporting\"):\n        comprehend = Comprehend(\"Amazon Comprehend\")\n        transcribe = Transcribe(\"Amazon Transcribe\")\n        report_system = Ansible(\"Incident Report\")\n\n    users >> api_gateway >> sqs >> lambda_function\n    lambda_function >> message_validity >> comprehend\n    message_validity >> transcribe_function >> transcribe\n    transcribe_function >> process_function >> report_system\n    support >> api_gateway\n",
            
            "outputText": "Enable AWS Auto Scaling for Lambda functions to handle varying support ticket loads efficiently. \nUse Amazon CloudWatch for monitoring and setting up alerts for API Gateway, SQS, and Lambda performance. \nImplement AWS Secrets Manager for securely storing API keys and authentication credentials. \nLeverage AWS Step Functions for better orchestration of message processing workflows. \nEnable AWS WAF to secure API Gateway endpoints against potential threats. \nUse Amazon Comprehend for sentiment analysis to prioritize high-severity support tickets."
          
          
    },
    {
        "id": 51,
        "inputCode": "import os\nfrom diagrams import Diagram,Cluster,Edge\nfrom diagrams.custom import Custom\nfrom diagrams.k8s.network import Service\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.analytics import Hadoop\nfrom diagrams.onprem.database import HBase\nfrom diagrams.onprem.monitoring import Grafana\nfrom diagrams.onprem.network import Apache,Nginx\nfrom urllib.request import urlretrieve\n\nopentsdb_url=\"https://avatars.githubusercontent.com/u/2086220?s=200&v=4\"\nopentsdb_icon=\"opentsdb.png\"\nimage_dir='images'\nos.makedirs(image_dir,exist_ok=True)\nurlretrieve(opentsdb_url,os.path.join(image_dir,opentsdb_icon))\n\ndiagram=Diagram(\"OpenTSDB on Kubernetes and HBase\",outformat=\"png\",show=not bool(os.environ.get('CI',0)))\nwith diagram:\n users=Users(\"Users-Quants,Devs,DevOps etc.\")\n tcollectors=Custom(\"TCollector agents metrics\",opentsdb_icon)\n with Cluster(\"Kubernetes on-prem\"):\n  grafana_ingress=Nginx(\"Nginx Ingress Grafana\")\n  opentsdb_ingress=Nginx(\"Nginx Ingress OpenTSDB\")\n  opentsdb_service=Service(\"OpenTSDB service\")\n  tcollectors>>opentsdb_ingress>>opentsdb_service\n  grafana_service=Service(\"Grafana service\")\n  grafana={}\n  httpd={}\n  for i in range(1,3):\n   with Cluster(f\"Grafana pod {i}\"):\n    httpd[i]=Apache(\"Apache httpd\\nkerberos proxy\")\n    grafana[i]=Grafana(\"Grafana\")\n    grafana_service>>httpd[i]>>grafana[i]>>opentsdb_service\n  users>>grafana_ingress>>grafana_service\n  opentsdb={}\n  for i in range(1,16):\n   opentsdb[i]=Custom(f\"OpenTSDB pod {i}\",opentsdb_icon)\n   opentsdb_service>>opentsdb[i]\n with Cluster(\"Hadoop cluster on-prem\"):\n  hbase={}\n  hdfs={}\n  for i in range(1,13):\n   with Cluster(f\"Hadoop node {i}\"):\n    hbase[i]=HBase(\"HBase\")\n    hdfs[i]=Hadoop(\"Hadoop HDFS\")\n    hbase[i]>>hdfs[i]\n    for j in range(1,16):\n     opentsdb[j]>>hbase[i]\n",
        "outputText": "Switch Apache HTTPD to NGINX or Envoy for better performance and Kubernetes-native features.Replace OpenTSDB with Prometheus + Thanos or VictoriaMetrics for better scalability and efficiency.Use MinIO, Ceph, or AWS S3 instead of Hadoop HDFS for cost-effective, scalable storage.Deploy HBase on a managed service (AWS DynamoDB, Bigtable, Azure CosmosDB) to reduce operational overhead.Enable Kubernetes HPA and Cluster Autoscaler to optimize resource usage.Use Istio or Linkerd for mTLS and service-to-service encryption to enhance security.Implement Web Application Firewall (WAF) to mitigate public-facing threats.Enforce Kubernetes RBAC and NetworkPolicies to restrict unauthorized access.Enable Prometheus with Thanos for long-term metrics storage and Grafana Loki for logging.Use OpenTelemetry for distributed tracing to monitor microservice performance"
      },
      {
        "id": 52,
        "inputCode": "import os\nfrom urllib.request import urlretrieve\nfrom diagrams import Diagram,Cluster\nfrom diagrams.aws.database import Aurora\nfrom diagrams.k8s.compute import Pod\nfrom diagrams.custom import Custom\n\nrabbitmq_url=\"https://jpadilla.github.io/rabbitmqapp/assets/img/icon.png\"\nrabbitmq_icon=\"rabbitmq.png\"\nimage_dir='images'\nos.makedirs(image_dir,exist_ok=True)\nurlretrieve(rabbitmq_url,os.path.join(image_dir,rabbitmq_icon))\n\ndiagram=Diagram(\"RabbitMQ Broker with custom icon\",outformat=\"png\",show=not bool(os.environ.get('CI',0)))\nwith diagram:\n with Cluster(\"Consumers\"):\n  consumers=[Pod(\"worker\"),Pod(\"worker\"),Pod(\"worker\")]\n rabbitmq=Custom(\"RabbitMQ\",rabbitmq_icon)\n rabbitmq>>consumers>>Aurora(\"Database\")\n",
        "outputText": "Consider switching from RabbitMQ to Apache Kafka or NATS if you need higher throughput, better scalability, or event-driven architecture with durable message storage. If you want to reduce operational overhead, using a managed message queue like AWS SQS, Google Pub/Sub, or Azure Service Bus can simplify maintenance and ensure high availability. To handle more users efficiently, deploy workers with Kubernetes Horizontal Pod Autoscaler (HPA) to dynamically scale based on queue depth. For better performance and lower latency, Redis Streams can be an alternative to RabbitMQ, especially for real-time event processing. Monitoring is crucial, so integrating Prometheus with a RabbitMQ exporter helps track queue depth, message rate, and worker performance. To prevent overload, implementing a Circuit Breaker pattern with Istio or Resilience4J ensures resilience against database failures. Security-wise, enforcing mTLS and Role-Based Access Control (RBAC) for RabbitMQ can prevent unauthorized access, while encrypting messages at rest and in transit safeguards sensitive data."
      },
      {
        "id": 53,
        "inputCode": "import os\nfrom diagrams import Diagram,Cluster,Edge\nfrom diagrams.gcp.compute import GKE\nfrom diagrams.gcp.network import FirewallRules,LoadBalancing\nfrom diagrams.gcp.storage import GCS\nfrom diagrams.k8s.compute import Pod\nfrom diagrams.k8s.network import Service\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.network import Nginx\nfrom diagrams.saas.cdn import Cloudflare\n\ngraph_attr={\"splines\":\"spline\"}\n\ndiagram=Diagram('GCP Cloudflare Web Architecture GKE',show=not bool(os.environ.get('CI',0)),direction='TB',filename='images/gcp_cloudflare_web_architecture_gke',graph_attr=graph_attr)\nwith diagram:\n users=Users(\"Internet Users\")\n with Cluster(\"Cloudflare\"):\n  dns=Cloudflare(\"Cloudflare\\nDNS\")\n  cdn=Cloudflare(\"Cloudflare\\nCDN / WAF\")\n  users>>Edge(label=\"DNS queries\")>>dns\n  users>>Edge(label=\"HTTPS traffic\")>>cdn\n with Cluster(\"Google Cloud\"):\n  firewall=FirewallRules(\"Firewall\")\n  load_balancer=LoadBalancing(\"Cloud Load Balancer\")\n  gcs=GCS(\"GCS bucket\\n(static assets)\")\n  cdn>>Edge(label=\"Proxied HTTPS Traffic\")>>firewall>>load_balancer\n  cdn>>gcs\n  with Cluster(\"Kubernetes Cluster\"):\n   eks=GKE(\"GKE\")\n   with Cluster(\"Ingress\"):\n    nginx=Nginx(\"Nginx\\nIngress Controller\")\n   with Cluster(\"WebApp 1\"):\n    service=Service(\"WebApp 1 Service\")\n    nginx>>service\n    pods=[service>>Pod(f\"Web {_}\") for _ in range(1,4)]\n   with Cluster(\"WebApp 2\"):\n    service=Service(\"WebApp 2 Service\")\n    nginx>>service\n    pods=[service>>Pod(f\"Web {_}\") for _ in range(1,4)]\n   load_balancer>>nginx\n",
  
        "outputText": "Consider switching from Apache HTTPD to NGINX or Envoy for better performance, lower resource consumption, and built-in load balancing features. Instead of GlassFish, consider using Spring Boot with embedded Tomcat or Quarkus for a more lightweight and scalable Java application server. For caching, replace Memcached with Redis to gain better persistence, replication, and built-in data structures. To improve database scalability, use a managed Cassandra service like AWS Keyspaces, Azure CosmosDB, or Google Bigtable to reduce operational overhead. Instead of an F5 Big-IP load balancer, consider cloud-native alternatives like AWS Elastic Load Balancing (ALB/NLB), HAProxy, or Traefik for better integration with modern infrastructure. Implement service discovery with Consul or Kubernetes Ingress to optimize multi-datacenter failover. Security-wise, enforce TLS encryption for all traffic, enable role-based access control (RBAC) for Cassandra, and use Web Application Firewall (WAF) to protect against malicious attacks."
      },
      {
        "id": 54,
        "inputCode": "import os\nfrom diagrams import Diagram,Cluster\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.analytics import Hadoop,Spark\n\ngraph_attr={\"splines\":\"spline\",\"landscape\":\"false\"}\n\ndiagram=Diagram('Spark on Hadoop',show=not bool(os.environ.get('CI',0)),direction='TB',filename='images/spark_hadoop',graph_attr=graph_attr)\nwith diagram:\n users=Users(\"Users - Quants, Devs, DevOps etc.\")\n with Cluster(\"Hadoop cluster on-prem\") as hadoop:\n  spark={}\n  hdfs={}\n  node_range=range(1,13,1)\n  for _ in reversed(node_range):\n   with Cluster(f\"Hadoop node {_}\"):\n    spark[_]=Spark(\"Spark\")\n    hdfs[_]=Hadoop(\"Hadoop HDFS\")\n    spark[_]>>hdfs[_]\n",
  
        "outputText": "Consider switching from on-prem Hadoop HDFS to cloud-based storage solutions like AWS S3, Azure Data Lake, or Google Cloud Storage for better scalability, availability, and cost efficiency. Instead of Spark on bare-metal or VMs, consider using Kubernetes-based Spark (Spark on K8s) to improve resource utilization and auto-scaling. If managing a Hadoop cluster is costly, Databricks or AWS EMR can replace self-managed Spark**, reducing operational overhead. For faster query performance, **Apache Iceberg or Delta Lake can enhance data lake capabilities** with ACID transactions. Security-wise, enable encryption for data at rest and in transit, implement fine-grained access control using Apache Ranger, and use IAM roles for secure access to cloud storage."
      },
      {
        "id": 55,
        "inputCode": "import os\nfrom diagrams import Diagram, Edge\nfrom diagrams.programming.flowchart import Action\n\ngraph_attr={\"splines\":\"spline\"}\n\ndiagram=Diagram('Release Straight to Production!',show=not bool(os.environ.get('CI',0)),direction='BT',graph_attr=graph_attr)\nwith diagram:\n prod=Action(\"Prod\")\n for _ in range(1,4,1):\n  Action(\"Dev\")>>Edge(label=f\"Feature {_}\")>>prod\n",
        "outputText": "Consider replacing direct-to-production releases with a modern CI/CD pipeline using tools like GitHub Actions, GitLab CI/CD, or Jenkins to enable automated testing and deployment. Introduce staging and canary deployments to reduce production risks. Instead of manually managing feature releases, use feature flags (LaunchDarkly, Unleash) to gradually roll out changes. For scalability, adopt blue-green or rolling deployments to ensure zero downtime. Infrastructure as Code (IaC) with Terraform or AWS CloudFormation can improve deployment consistency. Security-wise, enforce code reviews, automated security scans, and access controls to prevent vulnerabilities from reaching production."
      },
      {
        "id": 56,
        "inputCode": "import os\nfrom diagrams import Diagram, Cluster, Edge\nfrom diagrams.onprem.database import HBase\nfrom diagrams.onprem.monitoring import OpenTSDB\nfrom diagrams.k8s.compute import Pod\nfrom diagrams.k8s.network import Service\n\ngraph_attr={\"splines\":\"spline\"}\n\ndiagram=Diagram('OpenTSDB on Kubernetes with HBase',show=not bool(os.environ.get('CI',0)),direction='TB',graph_attr=graph_attr)\nwith diagram:\n users=Pod(\"Users\")\n with Cluster(\"Kubernetes Cluster\"):\n  opentsdb=OpenTSDB(\"OpenTSDB\")\n  service=Service(\"OpenTSDB Service\")\n  users>>Edge(label=\"send metrics\")>>service>>opentsdb\n  with Cluster(\"HBase Cluster\"):\n   hbase_master=HBase(\"HBase Master\")\n   hbase_regionservers=[]\n   for _ in range(1,4,1):\n    hbase_regionservers.append(HBase(f\"HBase RegionServer {_}\"))\n   opentsdb>>Edge(label=\"store metrics\")>>hbase_master>>hbase_regionservers\n",
        "outputText": "Consider replacing Traefik with NGINX Ingress Controller or Istio if you need better observability, security policies, and traffic management. Cloud Load Balancer can be replaced with Internal Load Balancer for cost efficiency if traffic is internal. Cloud DNS remains a solid choice, but integrating Amazon Route 53 or external DNS providers might provide better control over domain resolution. For scaling, implement Horizontal Pod Autoscaler (HPA) and Cluster Autoscaler to dynamically adjust resources. Use Pod Disruption Budgets and node affinity to improve resilience. Security concerns include mTLS between services (if using Istio), Web Application Firewall (WAF) for public endpoints, and Role-Based Access Control (RBAC) to limit Kubernetes access. Also, implement network policies to restrict pod communication to only necessary services, minimizing attack surfaces."
      },
      {
        "id": 57,
        "inputCode": "import os\nfrom diagrams import Diagram, Cluster, Edge\nfrom diagrams.gcp.compute import GKE\nfrom diagrams.gcp.network import FirewallRules, LoadBalancing\nfrom diagrams.gcp.storage import GCS\nfrom diagrams.k8s.compute import Pod\nfrom diagrams.k8s.network import Service\nfrom diagrams.onprem.client import Users\nfrom diagrams.onprem.network import Nginx\nfrom diagrams.saas.cdn import Cloudflare\n\ngraph_attr={'splines':'spline'}\n\ndiagram=Diagram('GCP Cloudflare Web Architecture GKE',show=not bool(os.environ.get('CI',0)),direction='TB',graph_attr=graph_attr)\nwith diagram:\n users=Users('Internet Users')\n with Cluster('Cloudflare'):\n  dns=Cloudflare('Cloudflare\\nDNS')\n  cdn=Cloudflare('Cloudflare\\nCDN / WAF')\n  users>>Edge(label='DNS queries')>>dns\n  users>>Edge(label='HTTPS traffic')>>cdn\n with Cluster('Google Cloud'):\n  firewall=FirewallRules('Firewall')\n  load_balancer=LoadBalancing('Cloud Load Balancer')\n  gcs=GCS('GCS bucket\\n(static assets)')\n  cdn>>Edge(label='Proxied HTTPS Traffic')>>firewall>>load_balancer\n  cdn>>gcs\n  with Cluster('Kubernetes Cluster'):\n   eks=GKE('GKE')\n   with Cluster('Ingress'):\n    nginx=Nginx('Nginx\\nIngress Controller')\n   with Cluster('WebApp 1'):\n    service=Service('WebApp 1 Service')\n    nginx>>service\n    for _ in range(1,4,1):\n     service>>Pod(f'Web {_}')\n   with Cluster('WebApp 2'):\n    service=Service('WebApp 2 Service')\n    nginx>>service\n    for _ in range(1,4,1):\n     service>>Pod(f'Web {_}')\n   load_balancer>>nginx",
        "outputText": "Consider replacing StatefulSet with Kubernetes Operators if the workload is database-heavy (e.g., MySQL, PostgreSQL, or Kafka). Operators provide better automation for scaling and failover. If using Persistent Volumes (PV) and Persistent Volume Claims (PVC) with a StorageClass (SC), consider migrating to Kubernetes-native storage solutions like Longhorn, Rook (Ceph), or Portworx for improved scalability and resilience. For better performance, use SSD-backed storage classes instead of default disk types, and implement ReadWriteMany (RWX) storage for workloads needing shared volumes (e.g., NFS, EFS, or CephFS). Scaling can be improved using sharding and replication at the database level and Horizontal Pod Autoscaler (HPA) with StatefulSets for read replicas. To make the diagram more understandable, explicitly label the type of application (e.g., 'Database Cluster') and show replication mechanisms. Cost reduction can be achieved by using regional disks for high availability instead of zonal disks and reducing over-provisioned storage by dynamically resizing PVs using Container Storage Interface (CSI) volume expansion. Security concerns include encrypting storage at rest (using Kubernetes Secrets for database credentials), network policies to restrict pod-to-pod communication, and RBAC permissions to prevent unauthorized PVC modifications. Also, ensure automated backups using Velero or Stash to avoid data loss risks."
      },
      {
        "id": 58,
        "inputCode": "import os\nfrom diagrams import Diagram, Cluster, Edge\nfrom diagrams.onprem.network import Kong\nfrom diagrams.onprem.certificates import CertManager, LetsEncrypt\nfrom diagrams.onprem.vcs import Github\nfrom diagrams.onprem.gitops import ArgoCD\nfrom diagrams.onprem.client import Users\nfrom diagrams.k8s.compute import Pod\nfrom diagrams.k8s.network import Ingress, Service\nfrom diagrams.aws.compute import EKS\nfrom diagrams.aws.network import ELB, Route53\n\ngraph_attr={'splines':'spline'}\n\ndiagram=Diagram('Kubernetes Kong API Gateway EKS',show=not bool(os.environ.get('CI',0)),direction='TB',graph_attr=graph_attr)\nwith diagram:\n letsencrypt=LetsEncrypt('LetsEncrypt Certificate Authority')\n users=Users('Users')\n github=Github('GitHub')\n with Cluster('AWS'):\n  elb=ELB('ELB Load Balancer')\n  route53=Route53('Route53 DNS')\n  elb-route53\n  users>>Edge(label='HTTPS traffic')>>elb\n  users>>Edge(label='DNS queries')>>route53\n  with Cluster('Kubernetes Cluster'):\n   eks=EKS('EKS')\n   with Cluster('Cert Manager'):\n    certmanager=CertManager('Cert Manager')\n   with Cluster('ArgoCD'):\n    argocd=ArgoCD('ArgoCD')\n   with Cluster('Ingress'):\n    kong=Kong('Kong API Gateway')\n    ingress=Ingress('Kubernetes Ingress')\n   with Cluster('WebApp 2'):\n    service=Service('WebApp 2 Service')\n    kong>>service\n    for _ in range(3,0,-1):\n     Pod(f'Pod {_}')<<service\n   with Cluster('WebApp 1'):\n    service=Service('WebApp 1 Service')\n    kong>>service\n    for _ in range(3,0,-1):\n     Pod(f'Pod {_}')<<service\n   elb>>Edge(label='HTTPS traffic')>>ingress\n   ingress-kong\n   letsencrypt>>Edge(label='ACME protocol\\ngenerated certificate',style='dashed')>>certmanager>>Edge(label='SSL\\ncert',style='dashed')>>ingress\n   github>>Edge(label='GitOps trigger',style='dashed')>>argocd>>Edge(label='k8s\\nyaml\\nmanifest\\nupdates',style='dashed')>>ingress",
        "outputText": "Instead of Cert Manager + Let's Encrypt, consider AWS Certificate Manager (ACM) for auto-renewal and tighter AWS integration. ArgoCD is great, but FluxCD is another option with native GitOps in AWS. Instead of Kong API Gateway, AWS API Gateway or NGINX Ingress Controller can offer better performance and cost savings. EKS with Fargate can reduce infrastructure costs by auto-scaling without managing worker nodes. For better scaling, use horizontal pod autoscalers (HPA) and cluster autoscalers. Security concerns include securing ingress with WAF, enforcing IAM roles, enabling mutual TLS, and restricting GitHub access to avoid supply chain attacks. Cost can be cut by using Spot Instances for EKS nodes and optimizing storage via EBS gp3 or S3 where applicable. Improve clarity by using Terraform or CDK for infra-as-code and proper documentation via Backstage or Confluence."
      },
      {
        "id": 59,
        "inputCode": "import os\nfrom diagrams import Diagram\nfrom diagrams.k8s.clusterconfig import HPA\nfrom diagrams.k8s.compute import Deployment, Pod, ReplicaSet\nfrom diagrams.k8s.network import Ingress, Service\n\ndiagram=Diagram('Kubernetes Deployment HPA Ingress',show=not bool(os.environ.get('CI',0)),filename='images/kubernetes_deployment_hpa_ingress')\nwith diagram:\n net=Ingress('Ingress\\nwww.domain.com')>>Service('Service')\n net>>[Pod('web1'),Pod('web2'),Pod('web3')]<<ReplicaSet('ReplicaSet')<<Deployment('Deployment')<<HPA('Horizontal Pod Autoscaler')",
        "outputText": "Instead of HPA alone, consider KEDA for event-driven autoscaling, reducing unnecessary pod creation. Ingress NGINX is good, but AWS ALB Ingress Controller or Istio provides better routing and security. Instead of ReplicaSet (rs) manually managed, let Deployment handle it completely to simplify operations. Use Cluster Autoscaler to dynamically adjust worker nodes. Security-wise, enable mTLS for pod communication, implement RBAC, and restrict ingress traffic with network policies. To reduce costs, consider Spot Instances, EBS gp3 for storage, and autoscaled node pools. For better clarity, use Infra-as-Code (Terraform/CDK) and monitoring with Prometheus/Grafana."
      },
      {
        "id": 60,
        "inputCode": "import os\nfrom diagrams import Diagram, Cluster, Edge\nfrom diagrams.onprem.ci import GithubActions\nfrom diagrams.onprem.vcs import Git, Github\nfrom diagrams.saas.chat import Slack\nfrom diagrams.programming.language import Python\nfrom diagrams.programming.flowchart import Document\nfrom diagrams.custom import Custom\n\ngraph_attr={'splines':'spline'}\nimage_dir='images'\n\ndiagram=Diagram('GitHub Actions CI/CD',show=not bool(os.environ.get('CI',0)),direction='LR',filename=os.path.join(image_dir,'github_actions_cicd'),graph_attr=graph_attr)\nwith diagram:\n hari=Custom('Hari\\nPythonista','hari.jpeg')\n git=Git('Git')\n hari>>Edge(label='midnight coding')>>[Custom('D2','d2_graphic.png'),Python('Python')]>>Edge(label='git commit')>>git\n with Cluster('GitHub'):\n  github_repo=Github('Diagrams-as-Code GitHub repo')\n  with Cluster('GitHub Actions CI/CD'):\n   python_workflow=GithubActions('Generate Python Images workflow')\n   d2_workflow=GithubActions('Generate D2 Images workflow')\n  readme=Document('README.md')\n  git>>Edge(label='git push')>>github_repo\n  github_repo>>Edge(label='*.py changes trigger workflow')>>python_workflow\n  github_repo>>Edge(label='*.d2 changes trigger workflow')>>d2_workflow\n  [python_workflow,d2_workflow]>>Edge(label='update diagrams')>>github_repo>>readme\n slack=Slack('Slack')\n hari>>Edge(label='Hey Guys, look what I did!')>>slack",
        "outputText": "Jenkins is banned, and GitHub Actions is a good choice. However, consider GitHub Actions + Terraform for infra automation. Instead of relying on direct Python scripts, use Dagger or Taskfile to standardize workflows. D2 for diagrams is great, but Mermaid.js in GitHub can provide better visualization. Add CodeQL and Trivy for security scanning to avoid vulnerabilities. Use GitHub Actions caching to speed up runs and self-hosted runners for cost savings. Improve visibility with Slack or Discord webhooks for notifications. To reduce costs, optimize workflows by using matrix builds and conditional triggers to avoid redundant runs."
      },
      {
        "id": 61,
        "inputCode": "import os\nfrom diagrams import Diagram, Cluster\nfrom diagrams.gcp.analytics import BigQuery, Dataflow, PubSub\nfrom diagrams.gcp.compute import AppEngine, Functions\nfrom diagrams.gcp.database import BigTable\nfrom diagrams.gcp.iot import IotCore\nfrom diagrams.gcp.storage import GCS\n\ngraph_attr={'splines':'spline'}\n\ndiagram=Diagram('GCP PubSub Analytics',show=not bool(os.environ.get('CI',0)),filename='images/gcp_pubsub_analytics',graph_attr=graph_attr)\nwith diagram:\n pubsub=PubSub('GCP Pub/Sub')\n with Cluster('Data Sources'):\n  [IotCore('IoT Core1'),IotCore('IoT Core2'),IotCore('IoT Core3')]>>pubsub\n with Cluster('Analytics Destinations'):\n  with Cluster('Streaming'):\n   dataflow=Dataflow('Data Flow')\n  with Cluster('Data Lake'):\n   dataflow>>[BigQuery('BigQuery'),GCS('GCS Storage')]\n  with Cluster('Event Driven'):\n   with Cluster('Processing'):\n    dataflow>>AppEngine('AppEngine')>>BigTable('BigTable')\n   with Cluster('Serverless'):\n    dataflow>>Functions('Cloud Functions')>>AppEngine('AppEngine')\n pubsub>>dataflow",
        "outputText": "GCP IoT Core is deprecated, so migrate to Cloud IoT or MQTT with Cloud Run. Instead of App Engine, use Cloud Run or GKE Autopilot for better autoscaling and flexibility. BigTable is powerful but costly, so consider Firestore or BigQuery streaming if workloads fit. Cloud Functions are good, but replace with Cloud Run for better cold-start performance. For scalability, implement Pub/Sub message ordering and Dead Letter Queues. Improve security by enabling IAM least privilege, VPC Service Controls, and Pub/Sub encryption keys. Reduce costs by auto-scaling Dataflow jobs, using Spot VMs, and optimizing storage tiers in GCS. For better clarity, use Terraform for infra automation and enable Cloud Logging & Monitoring."
      }
      
      
      


  ]
}
