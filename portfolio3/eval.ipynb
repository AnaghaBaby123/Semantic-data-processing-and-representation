{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 62, Perplexity: 7.1007\n",
      "ID: 63, Perplexity: 12.7885\n",
      "ID: 64, Perplexity: 11.0817\n",
      "ID: 65, Perplexity: 3.9705\n",
      "ID: 66, Perplexity: 4.7665\n",
      "\n",
      "Average Perplexity: 7.9416\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"gokul-pv/Llama-3.2-1B-Instruct-16bit-CodeArchitect\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def calculate_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    return torch.exp(loss).item()\n",
    "\n",
    "with open(\"data/codearchitect_val_data.json\", \"r\") as f:\n",
    "    data = json.load(f)[\"ValidationData\"]\n",
    "\n",
    "perplexities = []\n",
    "\n",
    "for sample in data:\n",
    "    sample_id = sample.get(\"id\", \"N/A\")\n",
    "    input_code = sample[\"inputCode\"]\n",
    "    perp = calculate_perplexity(input_code)\n",
    "    perplexities.append(perp)\n",
    "    print(f\"ID: {sample_id}, Perplexity: {perp:.4f}\")\n",
    "\n",
    "# Optionally, print the average perplexity over the validation set\n",
    "if perplexities:\n",
    "    avg_perplexity = sum(perplexities) / len(perplexities)\n",
    "    print(f\"\\nAverage Perplexity: {avg_perplexity:.4f}\")\n",
    "else:\n",
    "    print(\"No validation samples found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664bed3e2d9640aa97d90bb943a4929b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b7d024f8284614afd72884e8c55dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9465323c10ba41d1abf459407a1d265d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb1e1dc35904080950192db8ccba4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d74582046f64c0096e3b8f8bfa131fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.0040 (Range: 0 to 1, higher is better)\n",
      "Average ROUGE-1 F1 Score: 0.1536 (Range: 0 to 1, higher is better)\n",
      "Average ROUGE-L F1 Score: 0.1025 (Range: 0 to 1, higher is better)\n",
      "Average BERTScore F1: 0.4911 (Range: 0 to 1, higher is better)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score_fn\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"gokul-pv/Llama-3.2-1B-Instruct-16bit-CodeArchitect\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the validation data from a JSON file\n",
    "with open(\"data/codearchitect_val_data.json\", \"r\") as f:\n",
    "    data = json.load(f)[\"ValidationData\"]\n",
    "\n",
    "def evaluate_model(input_code, max_new_tokens=150, temperature=0.7):\n",
    "    # Tokenize the input code\n",
    "    inputs = tokenizer(input_code, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate output tokens\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id  # Avoid warnings in case the model doesn't have a pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the generated tokens into a string\n",
    "    prediction = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# Prepare lists to store metric scores for each sample\n",
    "bleu_scores = []\n",
    "rouge1_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "# Instantiate a ROUGE scorer (using stemmer for normalization)\n",
    "rouge_scorer_inst = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Use smoothing for BLEU to handle cases with very few tokens\n",
    "smoothing_fn = SmoothingFunction().method1\n",
    "\n",
    "# Store predictions and references for BERTScore calculation\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for sample in data:\n",
    "    input_code = sample[\"inputCode\"]\n",
    "    expected_output = sample[\"outputText\"]\n",
    "    \n",
    "    # Get model prediction\n",
    "    predicted_output = evaluate_model(input_code)\n",
    "    \n",
    "    # Append for BERTScore evaluation later\n",
    "    predictions.append(predicted_output)\n",
    "    references.append(expected_output)\n",
    "    \n",
    "    # Calculate BLEU score (tokenize by splitting on whitespace)\n",
    "    reference_tokens = expected_output.split()\n",
    "    candidate_tokens = predicted_output.split()\n",
    "    bleu = sentence_bleu(\n",
    "        [reference_tokens], \n",
    "        candidate_tokens, \n",
    "        smoothing_function=smoothing_fn\n",
    "    )\n",
    "    bleu_scores.append(bleu)\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    rouge_scores = rouge_scorer_inst.score(expected_output, predicted_output)\n",
    "    rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "    rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "\n",
    "# Calculate average BLEU and ROUGE scores across the validation set\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores) if rouge1_scores else 0\n",
    "avg_rougeL = sum(rougeL_scores) / len(rougeL_scores) if rougeL_scores else 0\n",
    "\n",
    "# Calculate BERTScore (F1 score) for the entire corpus\n",
    "P, R, F1 = bert_score_fn(predictions, references, lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "avg_bert = F1.mean().item()\n",
    "\n",
    "# Print out the results with the metric ranges and interpretation\n",
    "print(f\"Average BLEU Score: {avg_bleu:.4f} (Range: 0 to 1, higher is better)\")\n",
    "print(f\"Average ROUGE-1 F1 Score: {avg_rouge1:.4f} (Range: 0 to 1, higher is better)\")\n",
    "print(f\"Average ROUGE-L F1 Score: {avg_rougeL:.4f} (Range: 0 to 1, higher is better)\")\n",
    "print(f\"Average BERTScore F1: {avg_bert:.4f} (Range: 0 to 1, higher is better)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsd-sem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
